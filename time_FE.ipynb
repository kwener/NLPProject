{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPProject.ipynb  NLPProject3.ipynb manner_FE.ipynb   time_FE.ipynb\n",
      "NLPProject2.ipynb degree_FE.ipynb   place_FE.ipynb\n",
      "Notebook metadata fixed! You can now commit to GitHub.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from google.colab import drive\n",
    "\n",
    "# Get the notebook's filename (usually matches the GitHub repo name)\n",
    "!ls *.ipynb\n",
    "notebook_name = \"NLPProject.ipynb\"  # ‚Üê Replace with your filename\n",
    "\n",
    "# Load and fix the notebook\n",
    "with open(notebook_name, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Option A: Remove widgets metadata completely (recommended)\n",
    "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Option B: Or add the missing state key\n",
    "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "#     nb['metadata']['widgets']['state'] = {}\n",
    "\n",
    "# Save the fixed version\n",
    "with open(notebook_name, 'w') as f:\n",
    "    json.dump(nb, f)\n",
    "\n",
    "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/kierstenwener/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_with_time_ex = {}\n",
    "for f in fn.frames():\n",
    "    for x in f.FE:\n",
    "        if x == \"Time\":\n",
    "            frames_with_time_ex[f.name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, char_labels, offsets):\n",
    "    token_labels = []\n",
    "    for start, end in offsets:\n",
    "        if start == end:\n",
    "            token_labels.append(\"O\")  # Special tokens like [CLS], [SEP]\n",
    "        else:\n",
    "            # Majority vote over character labels inside the token span\n",
    "            span_labels = char_labels[start:end]\n",
    "            if all(lab == \"O\" for lab in span_labels):\n",
    "                token_labels.append(\"O\")\n",
    "            elif span_labels[0] == \"B-Time\":\n",
    "                token_labels.append(\"B-Time\")\n",
    "            else:\n",
    "                token_labels.append(\"I-Time\")\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "Input IDs: torch.Size([9013, 128])\n",
      "Attention Masks: torch.Size([9013, 128])\n",
      "Labels: torch.Size([9013, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.corpus import framenet as fn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Map BIO tags to IDs\n",
    "label2id = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Find frames that include \"Time\" as a frame element\n",
    "\n",
    "for name, frame in frames_with_time_ex.items():\n",
    "    # Print the frame name for reference\n",
    "    for lu in frame.lexUnit.values():\n",
    "        #print(f\"\\nLexical Unit: {lu['name']}\")\n",
    "        lu_data = fn.lu(lu['ID'])\n",
    "        for ex in lu_data['exemplars']:\n",
    "            text = ex['text']\n",
    "            char_labels = [\"O\"] * len(text)\n",
    "            has_time_fe = False\n",
    "\n",
    "            for fe in ex['FE']:\n",
    "                for i in fe:\n",
    "                    if i[2] == \"Time\":\n",
    "                        start, end = i[0], i[1]\n",
    "                        if start < end:\n",
    "                            char_labels[start] = \"B-Time\"\n",
    "                            for i in range(start+1, end):\n",
    "                                char_labels[i] = \"I-Time\"\n",
    "                            has_time_fe = True\n",
    "            if not has_time_fe:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Tokenize\n",
    "            tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # Map character-level labels to token-level labels\n",
    "            token_labels = align_labels_with_tokens(text, char_labels, offsets)\n",
    "            label2id_binary = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}  \n",
    "            # Pad remaining labels with -100 where attention mask is 0 (i.e., padding tokens)\n",
    "\n",
    "\n",
    "            label_ids = [label2id_binary.get(lab, 0) for lab in token_labels]\n",
    "            label_ids = [\n",
    "                label if mask == 1 else -100 \n",
    "                for label, mask in zip(label_ids, attention_mask)\n",
    "            ]\n",
    "            # Store tensors\n",
    "            input_ids_list.append(torch.tensor(input_ids))\n",
    "            attention_masks_list.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(label_ids))\n",
    "\n",
    "# Final dataset tensors\n",
    "input_ids_tensor = torch.stack(input_ids_list)\n",
    "attention_masks_tensor = torch.stack(attention_masks_list)\n",
    "labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "print(\"Tensor shapes:\")\n",
    "print(\"Input IDs:\", input_ids_tensor.shape)\n",
    "print(\"Attention Masks:\", attention_masks_tensor.shape)\n",
    "print(\"Labels:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "validation_split = 0.5\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Shuffle the data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation (without shuffling)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SubsetRandomSampler(range(len(val_dataset))),  # Don't shuffle validation data\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class FrameElementClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        #self.query_encoder = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.token_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    #def forward(self, input_ids, attention_mask, role_ids, role_mask):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode sentence\n",
    "        sentence_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = sentence_outputs.last_hidden_state  # shape: (B, T, H)\n",
    "\n",
    "        # Project sentence tokens\n",
    "        token_embeddings = self.token_projection(token_embeddings)  # shape: (B, T, H)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "\n",
    "        return logits  # Apply softmax for inference or use with CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "accuracies = []\n",
    "num_batches = 15\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FrameElementClassifier().to(device)\n",
    "#class_weights = torch.tensor([0.4, 0.6]).to(device)  # Make time more important\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 3), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def fix_bio_predictions2(predictions):\n",
    "    corrected = []\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sentence = predictions[i].tolist()\n",
    "        sentence_corrected = sentence.copy()\n",
    "\n",
    "        for j in range(seq_len-1):\n",
    "            if sentence[j] == 2:  # I-Time\n",
    "                if j == 0:\n",
    "                    # Beginning of sequence, can't be I-Time\n",
    "                    sentence_corrected[j] = 0\n",
    "\n",
    "                elif sentence[j-1] == 0:\n",
    "                    # Look ahead to see if more 2's follow\n",
    "                    if sentence[j+1] == 2:\n",
    "                        sentence_corrected[j-1] = 2\n",
    "                        sentence_corrected[j] = sentence[j]\n",
    "                        \n",
    "                    else: \n",
    "                        sentence_corrected[j] = 0\n",
    "                else:\n",
    "                    sentence_corrected[j] = sentence[j]\n",
    "            else:\n",
    "                sentence_corrected[j] = sentence[j]\n",
    "\n",
    "\n",
    "\n",
    "        corrected.append(sentence_corrected)\n",
    "\n",
    "    return torch.tensor(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        #gives index of highest scoring class\n",
    "        predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "        fixed_predictions = fix_bio_predictions2(predicted_tokens)\n",
    "\n",
    "        mask = (target_index != -100)\n",
    "\n",
    "        all_true_labels.extend(target_index[mask].view(-1).cpu().numpy())\n",
    "        all_pred_labels.extend(fixed_predictions[mask].view(-1).cpu().numpy())\n",
    "\n",
    "# Report\n",
    "print(\"Classification Report (O, B-Time, I-Time):\")\n",
    "print(classification_report(all_true_labels, all_pred_labels, target_names=['O', 'B-Time', 'I-Time']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Time FE Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_np = np.array(all_true_labels)\n",
    "pred_np = np.array(all_pred_labels)\n",
    "\n",
    "for label_id, label_name in enumerate(['O', 'B-Time', 'I-Time']):\n",
    "    total = np.sum(true_np == label_id)\n",
    "    correct = np.sum((true_np == label_id) & (pred_np == label_id))\n",
    "    print(f\"{label_name} Accuracy: {correct}/{total} = {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def evaluate_model_postprocessed(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels_all = []\n",
    "    pred_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            raw_preds = torch.argmax(outputs, dim=-1)  # (B, T)\n",
    "            fixed_preds = fix_bio_predictions2(raw_preds)  # Apply post-processing\n",
    "\n",
    "            for label_seq, pred_seq, mask in zip(labels, fixed_preds, attention_mask):\n",
    "                # Remove padding and apply attention mask\n",
    "                true_seq = [label.item() for label, m in zip(label_seq, mask) if m == 1 and label != -100]\n",
    "                pred_seq = [pred.item() for pred, m in zip(pred_seq, mask) if m == 1]\n",
    "\n",
    "                true_labels_all.append(true_seq)\n",
    "                pred_labels_all.append(pred_seq[:len(true_seq)])\n",
    "\n",
    "    return evaluate_predictions(true_labels_all, pred_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_bio_spans(label_seq):\n",
    "    spans = []\n",
    "    start = None\n",
    "\n",
    "    for i, label in enumerate(label_seq):\n",
    "        if label == 1:  # B\n",
    "            if start is not None:\n",
    "                spans.append((start, i - 1))  # close previous span\n",
    "            start = i\n",
    "        elif label == 2:\n",
    "            if start is None:\n",
    "                # Ill-formed BIO (I without B) ‚Äî treat as beginning a new span\n",
    "                start = i\n",
    "        else:  # label == 0\n",
    "            if start is not None:\n",
    "                spans.append((start, i - 1))\n",
    "                start = None\n",
    "\n",
    "    if start is not None:\n",
    "        spans.append((start, len(label_seq) - 1))\n",
    "    \n",
    "    return spans\n",
    "\n",
    "def evaluate_predictions(true_labels_list, pred_labels_list):\n",
    "    strict_match = 0\n",
    "    partial_match = 0\n",
    "    total_spans = 0\n",
    "\n",
    "    for true_seq, pred_seq in zip(true_labels_list, pred_labels_list):\n",
    "        true_spans = extract_bio_spans(true_seq)\n",
    "        pred_spans = extract_bio_spans(pred_seq)\n",
    "        total_spans += len(true_spans)\n",
    "\n",
    "        for t_start, t_end in true_spans:\n",
    "            t_range = set(range(t_start, t_end + 1))\n",
    "            match_found = False\n",
    "            for p_start, p_end in pred_spans:\n",
    "                p_range = set(range(p_start, p_end + 1))\n",
    "                if t_range == p_range:\n",
    "                    strict_match += 1\n",
    "                    match_found = True\n",
    "                    break\n",
    "                elif t_range & p_range:\n",
    "                    match_found = True\n",
    "            if match_found:\n",
    "                partial_match += 1\n",
    "\n",
    "    return {\n",
    "        \"Total Time Elements\": total_spans,\n",
    "        \"Strict Matches\": strict_match,\n",
    "        \"Partial Matches\": partial_match,\n",
    "        \"Strict Accuracy\": strict_match / total_spans if total_spans > 0 else 0,\n",
    "        \"Partial Accuracy\": partial_match / total_spans if total_spans > 0 else 0\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model_postprocessed(model, val_dataloader, device)\n",
    "\n",
    "print(\"üìä Post-Processed Evaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# 2. Prepare a test sentence\n",
    "text = \"They stayed at a cozy bed and breakfast overlooking the lake.\"\n",
    "\n",
    "# Tokenize with offsets to possibly map back later (optional here)\n",
    "encoding = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True,\n",
    "                     truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "input_ids = encoding[\"input_ids\"]        # shape: (1, 128)\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "# 3. Pass through the model\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask)  # shape: (1, 128, 3)\n",
    "    predictions2 = torch.argmax(logits, dim=-1)  # shape: (1, 128)\n",
    "    fixed_predictions = fix_bio_predictions2(predictions2)\n",
    "\n",
    "id2label = {0: \"O\", 1: \"B-Time\", 2: \"I-Time\"}\n",
    "predicted_tags2 = [id2label[i.item()] for i in fixed_predictions[0]]\n",
    "\n",
    "# 5. Get back tokens for visualization (optional)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "for tok, tag in zip(tokens, predicted_tags2):\n",
    "    print(f\"{tok:15} {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def fix_bio_predictions3(predictions):\n",
    "    corrected = []\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sentence = predictions[i].tolist()\n",
    "        sentence_corrected = sentence.copy()\n",
    "\n",
    "        for j in range(seq_len):\n",
    "            tag = sentence[j]\n",
    "\n",
    "            if tag == 2:  # I-tag\n",
    "                if j == 0:\n",
    "                    # Can't start with I -> make it O\n",
    "                    sentence_corrected[j] = 0\n",
    "\n",
    "                elif sentence_corrected[j - 1] == 0:\n",
    "                    # Pattern: O I ...\n",
    "                    # Check if a run of I-tags follows\n",
    "                    run_length = 1\n",
    "                    k = j + 1\n",
    "                    while k < seq_len and sentence[k] == 2:\n",
    "                        run_length += 1\n",
    "                        k += 1\n",
    "\n",
    "                    if run_length >= 1:\n",
    "                        # Change the O (at j-1) to B\n",
    "                        sentence_corrected[j - 1] = 1\n",
    "                    else:\n",
    "                        # Lone I -> make it O\n",
    "                        sentence_corrected[j] = 0\n",
    "\n",
    "        corrected.append(sentence_corrected)\n",
    "\n",
    "    return torch.tensor(corrected, device=predictions.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def evaluate_model_postprocessed2(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels_all = []\n",
    "    pred_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            raw_preds = torch.argmax(outputs, dim=-1)  # (B, T)\n",
    "            fixed_preds = fix_bio_predictions3(raw_preds)  # Apply post-processing\n",
    "\n",
    "            for label_seq, pred_seq, mask in zip(labels, fixed_preds, attention_mask):\n",
    "                # Remove padding and apply attention mask\n",
    "                true_seq = [label.item() for label, m in zip(label_seq, mask) if m == 1 and label != -100]\n",
    "                pred_seq = [pred.item() for pred, m in zip(pred_seq, mask) if m == 1]\n",
    "\n",
    "                true_labels_all.append(true_seq)\n",
    "                pred_labels_all.append(pred_seq[:len(true_seq)])\n",
    "\n",
    "    return evaluate_predictions(true_labels_all, pred_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = evaluate_model_postprocessed2(model, val_dataloader, device)\n",
    "\n",
    "print(\"üìä Post-Processed Evaluation Results:\")\n",
    "for k, v in results2.items():\n",
    "    print(f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# 2. Prepare a test sentence\n",
    "text = \"\"\n",
    "\n",
    "# Tokenize with offsets to possibly map back later (optional here)\n",
    "encoding = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True,\n",
    "                     truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "input_ids = encoding[\"input_ids\"]        # shape: (1, 128)\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "# 3. Pass through the model\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask)  # shape: (1, 128, 3)\n",
    "    predictions2 = torch.argmax(logits, dim=-1)  # shape: (1, 128)\n",
    "    fixed_preds = fix_bio_predictions3(predictions2)\n",
    "\n",
    "id2label = {0: \"O\", 1: \"B-Time\", 2: \"I-Time\"}\n",
    "predicted_tags2 = [id2label[i.item()] for i in fixed_preds[0]]\n",
    "\n",
    "# 5. Get back tokens for visualization (optional)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "for tok, tag in zip(tokens, predicted_tags2):\n",
    "    print(f\"{tok:15} {tag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
