{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "iP1t7fbpUmFs", "outputId": "7d4f4feb-5863-481b-dffb-dc8857ac932f"}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'google'", "output_type": "error", "traceback": ["\u001b[31m---------------------------------------------------------------------------\u001b[39m", "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Get the notebook's filename (usually matches the GitHub repo name)\u001b[39;00m\n\u001b[32m      5\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mls *.ipynb\u001b[39m\u001b[33m'\u001b[39m)\n", "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"]}], "source": ["import json\n", "#from google.colab import drive\n", "\n", "# Get the notebook's filename (usually matches the GitHub repo name)\n", "!ls *.ipynb\n", "notebook_name = \"NLPProject.ipynb\"  # \u2190 Replace with your filename\n", "\n", "# Load and fix the notebook\n", "with open(notebook_name, 'r') as f:\n", "    nb = json.load(f)\n", "\n", "# Option A: Remove widgets metadata completely (recommended)\n", "if 'metadata' in nb and 'widgets' in nb['metadata']:\n", "    del nb['metadata']['widgets']\n", "\n", "# Option B: Or add the missing state key\n", "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n", "#     nb['metadata']['widgets']['state'] = {}\n", "\n", "# Save the fixed version\n", "with open(notebook_name, 'w') as f:\n", "    json.dump(nb, f)\n", "\n", "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "JG9DVY8w3JSE", "outputId": "f5f0327a-345a-4398-9f95-27d4246d2728"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["[nltk_data] Downloading package framenet_v17 to\n", "[nltk_data]     /Users/kierstenwener/nltk_data...\n", "[nltk_data]   Package framenet_v17 is already up-to-date!\n"]}, {"data": {"text/plain": ["True"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["from pprint import pprint\n", "from operator import itemgetter\n", "import nltk\n", "from nltk.corpus import framenet as fn\n", "from nltk.corpus.reader.framenet import PrettyList\n", "nltk.download('framenet_v17')\n"]}, {"cell_type": "markdown", "metadata": {"id": "k4vUys7J3JSG"}, "source": ["Overview Notes\n", "\n", "-FE is a frame element, the the frame evpking words are the lexical units (LU)\n", "    -the concept of cooking involves the cook, the food that is being cooked, something holding the food while cooking, and the source of heat\n", "    -this is represented by a frame called Apply_Heat and the Cook, Food, Heating_Instrument, and COntainer are the FEs; the words that evoke the frame (like fry, bake, boil, broil, etc) are the LUs\n", "\n", "-inheritance: An IS-A relationship, the child frame is a subtype of the parent frame, each FE in the parent is bound to a corresponding FE in the child\n", "    -ex; the \"Revenge\" frame which inherits from the \"Rewards_and_punishments\" frame\n", "-using: The child frame presupposes the parent frame as background\n", "    -eg: the \"speed\" frame \"uses\" the \"Motion\" frame, but not all parent FEs need to be bound to child FEs\n", "-subframe: The child is a sibvent of a complex event represented by the parent\n", "    -eg: the \"Criminal_process\" frame has subframes \"arrest\", \"arraignment\", \"trial\" and \"sentencing\"\n", "-Perspective on: the child frame provices a particular perspective on an un-perspectivized parent frame; a pair of examples consists of \"Hiring\" and \"Get_a_job\" frames, which perspectivize the \"Employment_start\" frame from the Employers and Employees point of view"]}, {"cell_type": "markdown", "metadata": {"id": "MVkwEaRx3JSH"}, "source": ["-use frames() to get list of all frames; can get particular frame and info by using frame(); can provide a regular expressions to frames() to get list of all frames whoe names match the patern\n", "-can search Frames by their Lexical Units using frames_by_lemma(), which returns list of all frames that contain LUs in which the 'name' attrib of the LU matches the given regular expressiuon"]}, {"cell_type": "code", "execution_count": 111, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "cVW6en_N3JSH", "outputId": "78c4d7a8-8971-403b-a801-373c76284007"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["202\n", "Arrest\n", "Authorities charge a Suspect, who is under suspicion of having committed a crime (the Charges), and take him/her into custody.'The police arrested Harry on charges of manslaughter.'\n", "[<Parent=Intentionally_affect -- Inheritance -> Child=Arrest>, <Complex=Criminal_process -- Subframe -> Component=Arrest>, ...]\n"]}], "source": ["f = fn.frame(202)\n", "print(f.ID)\n", "print(f.name)\n", "print(f.definition)\n", "print(f.frameRelations)"]}, {"cell_type": "markdown", "metadata": {"id": "cyXoYfiB3JSI"}, "source": ["Lexical Unit overview\n", "- A lexical unit is a pairing of a word with a meaning\n", "-the \"Apply_Heat\" frame describes a common situation involving a cook, some food, and a heating instrument and is_envoked_ by words such as bake, blanch, broil, brown, simmer, steak, etc\n", "-The frame evoking words are the LUs in the Apply_head frame :\n", "    -can be adjs, nouns, or verbs\n", "    - reduction evokes \"Cause_change_of_scalar_position\" in ..\"the reduction of debt levels to 665 million from 2.6 billion\"\n", "-Framenet provides mult annotated examples of each sense of a word (i.e each LU)\n", "-each LU is linked to a frame, and the other words that evoke that frame\n", "    -framenet groups similar words together sematnically, like a thesaurous"]}, {"cell_type": "code", "execution_count": 112, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4jzN0tuk3JSI", "outputId": "c1e9a6dc-9de8-4286-b1b3-a37b04d86574"}, "outputs": [{"data": {"text/plain": ["[<lu ID=14733 name=a little.n>, <lu ID=14743 name=a little.adv>, ...]"]}, "execution_count": 112, "metadata": {}, "output_type": "execute_result"}], "source": ["#this is how you get details for a specific lexical unit\n", "PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')))"]}, {"cell_type": "markdown", "metadata": {"id": "OWPMrKHA3JSI"}, "source": ["-the LU named takes the form of the dotten string in which the lemme preceeds the\".\" and a part of speech follows the \".\""]}, {"cell_type": "code", "execution_count": 113, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Pivqlab23JSI", "outputId": "8caa1187-7f8f-448c-d5c9-c0b2bebe5f4c"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["foresee.v\n", "COD: be aware of beforehand; predict.\n", "Expectation\n", "foresee\n"]}], "source": ["print(fn.lu(256).name)\n", "print(fn.lu(256).definition)\n", "print(fn.lu(256).frame.name)\n", "print(fn.lu(256).lexemes[0].name)"]}, {"cell_type": "code", "execution_count": 113, "metadata": {"id": "gi5cY9cJ3JSJ"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "5XRtbary3JSJ"}, "source": ["get an annotated document with the docs() function"]}, {"cell_type": "code", "execution_count": 114, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "6jezcdzU3JSJ", "outputId": "9984d8fd-90df-40bd-83ef-894a961a4024"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["PropBank\n", "full-text sentence (2824246) in BellRinging:\n", "\n", "\n", "[POS] 17 tags\n", "\n", "[POS_tagset] PENN\n", "\n", "[text] + [annotationSet]\n", "\n", "`` I live in hopes that the ringers themselves will be drawn into\n", "             *****          *******                    *****     \n", "             Desir          Cause_t                    Cause     \n", "             [1]            [3]                        [2]       \n", "\n", " that fuller life .\n", "      ******\n", "      Comple\n", "      [4]   \n", " (Desir=Desiring, Cause_t=Cause_to_make_noise, Cause=Cause_motion, Comple=Completeness)\n", "\n"]}], "source": ["d = fn.docs('BellRinging')[0]\n", "print(d.corpname)\n", "print(d.sentence[49])"]}, {"cell_type": "markdown", "metadata": {"id": "DzT5pO4h3JSJ"}, "source": ["Below, you get information about the first annotation in the specificed sentence above. We see that the word hope evokes the frame desiring"]}, {"cell_type": "code", "execution_count": 115, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "EIXpu9R63JSJ", "outputId": "a199e43d-6650-4e88-9b82-ed7de87065f5"}, "outputs": [{"data": {"text/plain": ["annotation set (4528852):\n", "\n", "[status] MANUAL\n", "\n", "[LU] (6605) hope.n in Desiring\n", "\n", "[frame] (366) Desiring\n", "\n", "[GF] 2 relations\n", "\n", "[PT] 2 phrases\n", "\n", "[text] + [Target] + [FE] + [Noun]\n", "\n", "`` I live in hopes that the ringers themselves will be drawn into\n", "   - ^^^^ ^^ ***** ----------------------------------------------\n", "   E supp su       Event                                         \n", " \n", " that fuller life .\n", "-----------------\n", "                 \n", " (E=Experiencer, su=supp)\n"]}, "execution_count": 115, "metadata": {}, "output_type": "execute_result"}], "source": ["d.sentence[49].annotationSet[1]"]}, {"cell_type": "markdown", "metadata": {"id": "ji4mgbks3JSK"}, "source": ["find the training data :\n", "-this is a whole set of the training data, can index it to look at specific frames"]}, {"cell_type": "code", "execution_count": 116, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "aJPz3rY-3JSK", "outputId": "bf1424e2-08f5-4435-d311-e471290e217a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Number of frames: 1221\n"]}], "source": ["frames = fn.frames()\n", "print(f\"Number of frames: {len(frames)}\")"]}, {"cell_type": "markdown", "metadata": {"id": "9WYhklWf3JSK"}, "source": ["-frame elements are the specific participants, roles, or components of a frame;\n", "for each frame element,how many sample sentences do we have?"]}, {"cell_type": "code", "execution_count": 117, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "uVxOKrjo3JSK", "outputId": "0be6e991-3aa3-494a-99e7-82b08e1b87b8"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Frame Element: Time, Sample Sentences: 8170\n", "Frame Element: Manner, Sample Sentences: 7612\n", "Frame Element: Place, Sample Sentences: 7037\n", "Frame Element: Degree, Sample Sentences: 7012\n", "Frame Element: Means, Sample Sentences: 5045\n", "Frame Element: Explanation, Sample Sentences: 4539\n", "Frame Element: Depictive, Sample Sentences: 4091\n", "Frame Element: Purpose, Sample Sentences: 4091\n", "Frame Element: Circumstances, Sample Sentences: 3219\n", "Frame Element: Duration, Sample Sentences: 3120\n"]}], "source": ["frame_element_counts = {}\n", "#for each frame, loops through all frame elements\n", "for frame in fn.frames():\n", "    frame_name = frame.name\n", "\n", "    for fe_name, fe in frame.FE.items():\n", "\n", "        sample_sentences = frame.lexUnit\n", "        num_sentences = len(sample_sentences)\n", "\n", "        # Store the count of sentences for each frame element\n", "        if fe_name in frame_element_counts:\n", "            frame_element_counts[fe_name] += num_sentences  # Add the new count to the existing one\n", "        else:\n", "            frame_element_counts[fe_name] = num_sentences\n", "\n", "sorted_frame_elements = sorted(frame_element_counts.items(), key=lambda x: x[1], reverse=True)\n", "for fe_name, count in sorted_frame_elements[:10]:\n", "    print(f\"Frame Element: {fe_name}, Sample Sentences: {count}\")"]}, {"cell_type": "markdown", "metadata": {"id": "I-HaPnBx3JSK"}, "source": ["now we can say, given a sentence, which word corresponds to the \"\" element"]}, {"cell_type": "markdown", "metadata": {"id": "cAyAilyr3JSK"}, "source": ["-we could have a target frame element, get all of the sentences associated with that frame element.\n", "-First, we need to associate the sentence with a semantic frame, then parse to get the frame elements, or use bert to predict which word corresponds to a specific frame element\n", "\n", "https://arxiv.org/html/2502.12516v1\n", "-Json based formats show superior results, wrap target or word phrase in **\n"]}, {"cell_type": "code", "execution_count": 117, "metadata": {"id": "nztw1ZTJ3JSK"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": 118, "metadata": {"id": "3FSSHKpr3JSL"}, "outputs": [], "source": ["from transformers import pipeline"]}, {"cell_type": "code", "execution_count": 119, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "wQvXC1-N3JSL", "outputId": "21379e0f-25f5-4156-b5ba-0ec2eee5b1c3"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["2.6.0+cu124\n", "False\n"]}], "source": ["import torch\n", "print(torch.__version__)\n", "print(torch.cuda.is_available())  # Should return True if you have a CUDA-compatible GPU\n"]}, {"cell_type": "markdown", "metadata": {"id": "LSvfH1zj3Px1"}, "source": ["### TAGGING\n", "\n", "Started tagging using the listed top ten words. Added a way for context to be acknowledged while attempting to tag."]}, {"cell_type": "code", "execution_count": 190, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["a9ffa758dbe4473a9bcc2ab11f81d65a", "fcfb603c91a7477fbca8b2a42315fbe5", "f0d0339e036940f2898e1f4ab4595386", "ccd1268bb449424597e1adbc2759eaac", "eba2278d50db4c9eb9f358e7c4bf04e4", "27b3f00588b5471599138b87fa54e84e", "daaa7195586942a7a6d3102c69d31643", "7d9cc2081a4e4e44a0407bf17b6039e0", "9bf7d9321671462daabe3e2e3c0e382f", "00ffddc6ab474a169970f0a73f479a5e", "521b64995bcd446196edd2e5e6fb9596", "87d74c2fc4fb4d51b5342d46abec1ff6", "48732b2e1b7e41eea3e7cc9291bc366b", "63f5326581e04ce3bc87f2d146a47452", "0e497277268740bc962c815600a98d96", "6d064d800aa44e9f8f73b922dd684a3e", "8b86fdfc4fde4448b52b10e60299e2a9", "5fdd2a6287bd440ba733d9c7f540d966", "485c74b8cbf049ce8eaf9cea8252cc0d", "c7427ab411d44820a8f52e1a1a7ccb8d", "a0c6982846344f7f91eab850e01da542", "ec4b0ec408f144a187315eb5908d4476", "f3777b23abed4b0fb277f1ae11ff9778", "76cc152db024424f84dcf8845fff0fd7", "c35f6a3d046b4d7981ca5f755c91d101", "3eb95d534d054d7db602498c0c17cb8b", "49e9396e760b49699f02984e248838a7", "3c42ae0d1e4640fead714eeb32dab7c1", "f91e9a92f8014aba8620895e641fa3a5", "e5bbf29652054d5a87ca0eb6718e1ce9", "cd6f7efae6e04a20bdf7c5292b90ba73", "d1ac87d962d44a3180ea9877127311a8", "a48a57b8c67049028c66b3b3afa95d69", "1549cb39813f4bb1ae8afe3a1732f539", "2ab0e6c9897343e2b8f2083f9dae2783", "3c686ab14fcb4524ad4bcdb7d3669f10", "0916aa3a6f5a4e3f8777a867422f164b", "c9beac0970264650b34da4d9b6aa016d", "2c56e54cf1cb4c9781c477b69a5cdfaa", "0503a1b8defa4025bef3fa80e71923e5", "dac43819305043198339b70a318983a6", "97fdcedd9df64e61a3fa5791b6c30f18", "d328eaac875f4267837cbe9e2e81709a", "8f679e60e5364bc6b3207dbdd9bbf3ae", "9a66994dbe5c44e581e5ffd06ef1a62f", "9011a3a968d14321ae2466e92330da25", "ae1c152743004e91be70ce02c9f147e2", "c2cdc0c272564ddaa2d9a943f8b493c3", "09bd25c862ee47c9947992f71f7cad19", "dec9c30ea6cf44f2aee631a460675e69", "9738a557b50a49698a7c09e2db335d20", "8f4183cc7a824edf8d9f3d85f89e8ef7", "ded3670f3094494baf4157e35b0c01ed", "a132a3be264842bc91ffbaaa8e46ca67", "c176d10a687545fc93ce7c9d6ec7cc83"]}, "id": "FARqdvtx499q", "outputId": "bfc124ca-7491-40e7-9396-ce3d4fe6dea4"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n", "[nltk_data]     /root/nltk_data...\n", "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n", "[nltk_data]       date!\n", "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "a9ffa758dbe4473a9bcc2ab11f81d65a", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "87d74c2fc4fb4d51b5342d46abec1ff6", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f3777b23abed4b0fb277f1ae11ff9778", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1549cb39813f4bb1ae8afe3a1732f539", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n", "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9a66994dbe5c44e581e5ffd06ef1a62f", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Original Sentence: The meeting will begin at 3 PM.\n", "Tagged Sentence: [('The', 'none'), ('meeting', 'Place'), ('will', 'none'), ('begin', 'Time'), ('at', 'Purpose'), ('3', 'none'), ('PM', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: She worked all night to finish the project.\n", "Tagged Sentence: [('She', 'none'), ('worked', 'none'), ('all', 'none'), ('night', 'none'), ('to', 'none'), ('finish', 'Time'), ('the', 'none'), ('project', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The concert starts tomorrow.\n", "Tagged Sentence: [('The', 'none'), ('concert', 'none'), ('starts', 'none'), ('tomorrow', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: She explained the concept clearly.\n", "Tagged Sentence: [('She', 'none'), ('explained', 'none'), ('the', 'none'), ('concept', 'none'), ('clearly', 'Degree'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The team worked together efficiently.\n", "Tagged Sentence: [('The', 'none'), ('team', 'none'), ('worked', 'none'), ('together', 'Manner'), ('efficiently', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: He ran away nervously.\n", "Tagged Sentence: [('He', 'none'), ('ran', 'none'), ('away', 'none'), ('nervously', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: They visited the park in the morning.\n", "Tagged Sentence: [('They', 'none'), ('visited', 'none'), ('the', 'none'), ('park', 'Place'), ('in', 'Purpose'), ('the', 'none'), ('morning', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: I left my keys on the kitchen counter.\n", "Tagged Sentence: [('I', 'none'), ('left', 'none'), ('my', 'none'), ('keys', 'none'), ('on', 'Means'), ('the', 'none'), ('kitchen', 'Place'), ('counter', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The store is near the bus station.\n", "Tagged Sentence: [('The', 'none'), ('store', 'Place'), ('is', 'none'), ('near', 'none'), ('the', 'none'), ('bus', 'Place'), ('station', 'Circumstances'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: It was so hot that I couldn\u2019t stand outside.\n", "Tagged Sentence: [('It', 'none'), ('was', 'none'), ('so', 'Manner'), ('hot', 'Degree'), ('that', 'none'), ('I', 'none'), ('couldn', 'none'), ('\u2019', 'none'), ('t', 'none'), ('stand', 'Place'), ('outside', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: He\u2019s extremely talented.\n", "Tagged Sentence: [('He', 'none'), ('\u2019', 'none'), ('s', 'none'), ('extremely', 'none'), ('talented', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The cake was quite delicious.\n", "Tagged Sentence: [('The', 'none'), ('cake', 'none'), ('was', 'none'), ('quite', 'none'), ('delicious', 'Degree'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: She achieved her goals through hard work.\n", "Tagged Sentence: [('She', 'none'), ('achieved', 'none'), ('her', 'none'), ('goals', 'none'), ('through', 'none'), ('hard', 'Degree'), ('work', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: We traveled by car across the country.\n", "Tagged Sentence: [('We', 'none'), ('traveled', 'none'), ('by', 'Means'), ('car', 'none'), ('across', 'none'), ('the', 'none'), ('country', 'Circumstances'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The results were obtained using advanced techniques.\n", "Tagged Sentence: [('The', 'none'), ('results', 'none'), ('were', 'none'), ('obtained', 'none'), ('using', 'none'), ('advanced', 'Degree'), ('techniques', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: He explained why he was late for the meeting.\n", "Tagged Sentence: [('He', 'none'), ('explained', 'none'), ('why', 'none'), ('he', 'none'), ('was', 'none'), ('late', 'Manner'), ('for', 'Means'), ('the', 'none'), ('meeting', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The teacher clarified the rules of the game.\n", "Tagged Sentence: [('The', 'none'), ('teacher', 'Place'), ('clarified', 'none'), ('the', 'none'), ('rules', 'none'), ('of', 'Circumstances'), ('the', 'none'), ('game', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: She gave a detailed explanation about the new policy.\n", "Tagged Sentence: [('She', 'none'), ('gave', 'none'), ('a', 'none'), ('detailed', 'none'), ('explanation', 'Place'), ('about', 'none'), ('the', 'none'), ('new', 'Duration'), ('policy', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: She looked tired after the long trip.\n", "Tagged Sentence: [('She', 'none'), ('looked', 'none'), ('tired', 'Duration'), ('after', 'none'), ('the', 'none'), ('long', 'Duration'), ('trip', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The sky was clear and blue.\n", "Tagged Sentence: [('The', 'none'), ('sky', 'none'), ('was', 'none'), ('clear', 'Depictive'), ('and', 'none'), ('blue', 'Degree'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The dog seemed excited and playful.\n", "Tagged Sentence: [('The', 'none'), ('dog', 'Place'), ('seemed', 'none'), ('excited', 'Degree'), ('and', 'none'), ('playful', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: He went to the store to buy groceries.\n", "Tagged Sentence: [('He', 'none'), ('went', 'none'), ('to', 'none'), ('the', 'none'), ('store', 'Place'), ('to', 'none'), ('buy', 'Time'), ('groceries', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The project was created to improve customer service.\n", "Tagged Sentence: [('The', 'none'), ('project', 'Place'), ('was', 'none'), ('created', 'none'), ('to', 'none'), ('improve', 'Time'), ('customer', 'none'), ('service', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: She studies hard to become a doctor.\n", "Tagged Sentence: [('She', 'none'), ('studies', 'none'), ('hard', 'Time'), ('to', 'none'), ('become', 'Time'), ('a', 'none'), ('doctor', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: Despite the rain, they went for a walk.\n", "Tagged Sentence: [('Despite', 'none'), ('the', 'none'), ('rain', 'Place'), (',', 'none'), ('they', 'none'), ('went', 'none'), ('for', 'Means'), ('a', 'none'), ('walk', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: Under normal circumstances, the event would be postponed.\n", "Tagged Sentence: [('Under', 'none'), ('normal', 'Degree'), ('circumstances', 'none'), (',', 'none'), ('the', 'none'), ('event', 'Place'), ('would', 'none'), ('be', 'Time'), ('postponed', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: In the event of a fire, use the emergency exit.\n", "Tagged Sentence: [('In', 'Purpose'), ('the', 'none'), ('event', 'Place'), ('of', 'Circumstances'), ('a', 'none'), ('fire', 'Place'), (',', 'none'), ('use', 'Time'), ('the', 'none'), ('emergency', 'Place'), ('exit', 'Place'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: He lived in Paris for two years.\n", "Tagged Sentence: [('He', 'none'), ('lived', 'none'), ('in', 'Purpose'), ('Paris', 'none'), ('for', 'Means'), ('two', 'none'), ('years', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: The lecture lasted for three hours.\n", "Tagged Sentence: [('The', 'none'), ('lecture', 'Place'), ('lasted', 'none'), ('for', 'Means'), ('three', 'none'), ('hours', 'none'), ('.', 'none')]\n", "--------------------------------------------------\n", "Original Sentence: She was absent for a week due to illness.\n", "Tagged Sentence: [('She', 'none'), ('was', 'none'), ('absent', 'Time'), ('for', 'Means'), ('a', 'none'), ('week', 'none'), ('due', 'none'), ('to', 'none'), ('illness', 'Duration'), ('.', 'none')]\n", "--------------------------------------------------\n"]}], "source": ["\n", "\n", "import nltk\n", "from nltk.tokenize import word_tokenize\n", "from nltk.corpus import framenet as fn\n", "nltk.download('averaged_perceptron_tagger_eng')\n", "import re\n", "from nltk import pos_tag\n", "from transformers import BertTokenizer, BertModel\n", "\n", "#  pre-trained BERT model and tokenizer ( this will provide context for example sentences)\n", "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n", "bert_model = BertModel.from_pretrained('bert-base-uncased')\n", "\n", "top_10_fes = [\n", "    \"Time\", \"Manner\", \"Place\", \"Degree\", \"Means\", \"Explanation\",\n", "    \"Depictive\", \"Purpose\", \"Circumstances\", \"Duration\"\n", "]\n", "\n", "# dictionary to store lexical units for the top 10 frame elements\n", "frame_element_lexemes = {}\n", "\n", "# function to strip POS ( parts-of-speech) tags from lexemes\n", "def strip_pos_tag(lexeme):\n", "    return re.sub(r'\\.[a-z]+$', '', lexeme)\n", "\n", "# loop through all frames in FrameNet to gather lexical units for each of the top 10 frame elements\n", "for frame in fn.frames():\n", "    frame_name = frame.name  # Frame name (e.g., \"Cause\", \"Motion\")\n", "\n", "    # loop through each frame element in the frame\n", "    for fe_name, fe in frame.FE.items():\n", "        if fe_name in top_10_fes:\n", "            # get all lexical units for this frame element\n", "            if fe_name not in frame_element_lexemes:\n", "                frame_element_lexemes[fe_name] = set()\n", "            # if it's a lexical unit object, access its lexemes\n", "            for lu in frame.lexUnit:\n", "                if hasattr(lu, 'lexemes'):\n", "                    for lexeme in lu.lexemes:\n", "                        lexeme_name = strip_pos_tag(lexeme.name.lower())  # remove POS tag\n", "                        frame_element_lexemes[fe_name].add(lexeme_name)  # store lexemes in lowercase without POS tag\n", "                else:\n", "                    # if it's just a string, we directly add it as a lexeme\n", "                    lexeme_name = strip_pos_tag(lu.lower())  # remove POS tag\n", "                    frame_element_lexemes[fe_name].add(lexeme_name)\n", "\n", "# function to get BERT embeddings for a sentence\n", "def get_bert_embeddings(sentence):\n", "    # tokenize and encode sentence\n", "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n", "\n", "    # get the embeddings from BERT\n", "    with torch.no_grad():\n", "        outputs = bert_model(**inputs)\n", "\n", "    # return the token embeddings\n", "    return outputs.last_hidden_state\n", "\n", "# function to tag a sentence with context\n", "def tag_sentence_with_context(sentence):\n", "    tokens = word_tokenize(sentence)\n", "    token_embeddings = get_bert_embeddings(sentence)\n", "\n", "    tagged_sentence = []\n", "    pos_tags = pos_tag(tokens)  # POS tagging to help with filtering\n", "\n", "    # loop through each word in the sentence\n", "    for i, (word, pos) in enumerate(pos_tags):\n", "        word_lower = strip_pos_tag(word.lower())  # Convert word to lowercase and strip POS tag\n", "        assigned_fe = \"none\"\n", "\n", "        # check each top frame element for the word\n", "        for fe_name, lexemes in frame_element_lexemes.items():\n", "            if word_lower in lexemes:\n", "                # use BERT embeddings and POS tags to determine if the word is related to the correct FE\n", "                embedding = token_embeddings[0, i].cpu().numpy()\n", "\n", "                # apply conditions to check if the word is related to the correct FE based on its embedding and POS tag\n", "                if fe_name == \"Time\" and pos in ['NN', 'NNS', 'VB', 'VBP']:  # time is often linked to nouns/verbs\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Place\" and pos in ['NN', 'NNS']:  # place-related words are usually nouns\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Degree\" and pos in ['RB', 'JJ']:  # degree-related words are often adverbs or adjectives\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Manner\" and pos in ['RB']:  # manner-related words are adverbs\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Means\" and pos in ['NN', 'NNS', 'IN']:  # means-related words are nouns or prepositions\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Purpose\" and pos in ['NN', 'VB', 'IN']:  # purpose-related words are nouns, verbs, or prepositions\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Explanation\" and pos in ['NN', 'VB']:  # explanation-related words are nouns or verbs\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Circumstances\" and pos in ['NN', 'IN', 'JJ', 'RB']:  # circumstances-related words are nouns, prepositions, adjectives, or adverbs\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Depictive\" and pos in ['JJ', 'RB']:  # depictive-related words are adjectives or adverbs\n", "                    assigned_fe = fe_name\n", "                    break\n", "                elif fe_name == \"Duration\" and pos in ['NN', 'VB', 'JJ', 'RB']:  # duration-related words are nouns, verbs, adjectives, or adverbs\n", "                    assigned_fe = fe_name\n", "                    break\n", "\n", "        tagged_sentence.append((word, assigned_fe))\n", "\n", "    return tagged_sentence\n", "\n", "# Example sentences for testing\n", "sentences = [\n", "    \"The meeting will begin at 3 PM.\",\n", "    \"She worked all night to finish the project.\",\n", "    \"The concert starts tomorrow.\",\n", "    \"She explained the concept clearly.\",\n", "    \"The team worked together efficiently.\",\n", "    \"He ran away nervously.\",\n", "    \"They visited the park in the morning.\",\n", "    \"I left my keys on the kitchen counter.\",\n", "    \"The store is near the bus station.\",\n", "    \"It was so hot that I couldn\u2019t stand outside.\",\n", "    \"He\u2019s extremely talented.\",\n", "    \"The cake was quite delicious.\",\n", "    \"She achieved her goals through hard work.\",\n", "    \"We traveled by car across the country.\",\n", "    \"The results were obtained using advanced techniques.\",\n", "    \"He explained why he was late for the meeting.\",\n", "    \"The teacher clarified the rules of the game.\",\n", "    \"She gave a detailed explanation about the new policy.\",\n", "    \"She looked tired after the long trip.\",\n", "    \"The sky was clear and blue.\",\n", "    \"The dog seemed excited and playful.\",\n", "    \"He went to the store to buy groceries.\",\n", "    \"The project was created to improve customer service.\",\n", "    \"She studies hard to become a doctor.\",\n", "    \"Despite the rain, they went for a walk.\",\n", "    \"Under normal circumstances, the event would be postponed.\",\n", "    \"In the event of a fire, use the emergency exit.\",\n", "    \"He lived in Paris for two years.\",\n", "    \"The lecture lasted for three hours.\",\n", "    \"She was absent for a week due to illness.\"\n", "]\n", "\n", "# process each sentence and tag it\n", "tagged_sentences = []\n", "\n", "for sentence in sentences:\n", "    tagged = tag_sentence_with_context(sentence)\n", "    tagged_sentences.append(tagged)\n", "\n", "# print and test sentences\n", "for sentence, tagged in zip(sentences, tagged_sentences):\n", "    print(f\"Original Sentence: {sentence}\")\n", "    print(f\"Tagged Sentence: {tagged}\")\n", "    print(\"-\" * 50)\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "LV4tIj4N3Sua"}, "source": []}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.13.2"}}, "nbformat": 4, "nbformat_minor": 0}