{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPProject.ipynb  NLPProject2.ipynb\n",
      "Notebook metadata fixed! You can now commit to GitHub.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from google.colab import drive\n",
    "\n",
    "# Get the notebook's filename (usually matches the GitHub repo name)\n",
    "!ls *.ipynb\n",
    "notebook_name = \"NLPProject.ipynb\"  # ‚Üê Replace with your filename\n",
    "\n",
    "# Load and fix the notebook\n",
    "with open(notebook_name, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Option A: Remove widgets metadata completely (recommended)\n",
    "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Option B: Or add the missing state key\n",
    "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "#     nb['metadata']['widgets']['state'] = {}\n",
    "\n",
    "# Save the fixed version\n",
    "with open(notebook_name, 'w') as f:\n",
    "    json.dump(nb, f)\n",
    "\n",
    "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/kierstenwener/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Element: Time, Sample Sentences: 8170\n",
      "Frame Element: Manner, Sample Sentences: 7612\n",
      "Frame Element: Place, Sample Sentences: 7037\n",
      "Frame Element: Degree, Sample Sentences: 7012\n",
      "Frame Element: Means, Sample Sentences: 5045\n",
      "Frame Element: Explanation, Sample Sentences: 4539\n",
      "Frame Element: Depictive, Sample Sentences: 4091\n",
      "Frame Element: Purpose, Sample Sentences: 4091\n",
      "Frame Element: Circumstances, Sample Sentences: 3219\n",
      "Frame Element: Duration, Sample Sentences: 3120\n"
     ]
    }
   ],
   "source": [
    "frame_element_counts = {}\n",
    "#for each frame, loops through all frame elements\n",
    "for frame in fn.frames():\n",
    "    frame_name = frame.name\n",
    "\n",
    "    for fe_name, fe in frame.FE.items():\n",
    "\n",
    "        sample_sentences = frame.lexUnit\n",
    "        num_sentences = len(sample_sentences)\n",
    "\n",
    "        # Store the count of sentences for each frame element\n",
    "        if fe_name in frame_element_counts:\n",
    "            frame_element_counts[fe_name] += num_sentences  # Add the new count to the existing one\n",
    "        else:\n",
    "            frame_element_counts[fe_name] = num_sentences\n",
    "\n",
    "sorted_frame_elements = sorted(frame_element_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for fe_name, count in sorted_frame_elements[:10]:\n",
    "    print(f\"Frame Element: {fe_name}, Sample Sentences: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_with_time_ex = {}\n",
    "for f in fn.frames():\n",
    "    for x in f.FE:\n",
    "        if x == \"Time\":\n",
    "            frames_with_time_ex[f.name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, char_labels, offsets):\n",
    "    token_labels = []\n",
    "    for (token_start, token_end) in offsets:\n",
    "        # For special tokens like [CLS] and [SEP], offset is usually (0,0)\n",
    "        if token_start == token_end:\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # If any character in the token is marked as Time,\n",
    "            # decide on a label for the entire token.\n",
    "            token_tag = \"O\"\n",
    "            for pos in range(token_start, token_end):\n",
    "                if pos < len(char_labels) and char_labels[pos] != \"O\":\n",
    "                    token_tag = char_labels[pos]\n",
    "                    break\n",
    "            token_labels.append(token_tag)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kierstenwener/Desktop/NLPProject/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "Input IDs: torch.Size([9013, 128])\n",
      "Attention Masks: torch.Size([9013, 128])\n",
      "Labels: torch.Size([9013, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.corpus import framenet as fn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Map BIO tags to IDs\n",
    "label2id = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Find frames that include \"Time\" as a frame element\n",
    "\n",
    "for name, frame in frames_with_time_ex.items():\n",
    "    # Print the frame name for reference\n",
    "    for lu in frame.lexUnit.values():\n",
    "        #print(f\"\\nLexical Unit: {lu['name']}\")\n",
    "        lu_data = fn.lu(lu['ID'])\n",
    "        for ex in lu_data['exemplars']:\n",
    "            text = ex['text']\n",
    "            char_labels = [\"O\"] * len(text)\n",
    "            has_time_fe = False\n",
    "\n",
    "            for fe in ex['FE']:\n",
    "                for i in fe:\n",
    "                    if i[2] == \"Time\":\n",
    "                        start, end = i[0], i[1]\n",
    "                        if start < end:\n",
    "                            char_labels[start] = \"B-Time\"\n",
    "                            for i in range(start+1, end):\n",
    "                                char_labels[i] = \"I-Time\"\n",
    "                            has_time_fe = True\n",
    "            if not has_time_fe:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Tokenize\n",
    "            tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # Map character-level labels to token-level labels\n",
    "            token_labels = align_labels_with_tokens(text, char_labels, offsets)\n",
    "            label2id_binary = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 1}  # Map both B-Time and I-Time to 1\n",
    "            # Pad remaining labels with -100 where attention mask is 0 (i.e., padding tokens)\n",
    "\n",
    "\n",
    "            label_ids = [label2id_binary.get(lab, 0) for lab in token_labels]\n",
    "            label_ids = [\n",
    "                label if mask == 1 else -100 \n",
    "                for label, mask in zip(label_ids, attention_mask)\n",
    "            ]\n",
    "            # Store tensors\n",
    "            input_ids_list.append(torch.tensor(input_ids))\n",
    "            attention_masks_list.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(label_ids))\n",
    "\n",
    "# Final dataset tensors\n",
    "input_ids_tensor = torch.stack(input_ids_list)\n",
    "attention_masks_tensor = torch.stack(attention_masks_list)\n",
    "labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "print(\"Tensor shapes:\")\n",
    "print(\"Input IDs:\", input_ids_tensor.shape)\n",
    "print(\"Attention Masks:\", attention_masks_tensor.shape)\n",
    "print(\"Labels:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: when\n"
     ]
    }
   ],
   "source": [
    "indices = (labels_tensor == 1).nonzero(as_tuple=False)\n",
    "sample_idx, token_idx = indices[0].tolist()\n",
    "token_id = input_ids_tensor[sample_idx][token_idx]\n",
    "token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "print(f\"Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'she', 'had', 'seen', 'no', 'reason', 'to', 'abandon', 'it', 'when', 'she', 'came', 'to', 'med', '##ew', '##ich', 'two', 'years', 'ago', ',', 'even', 'though', 'she', 'might', 'now', 'have', 'been', 'able', 'to', 'afford', 'a', 'car', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor[sample_idx])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           -> 0\n",
      "she             -> 0\n",
      "had             -> 0\n",
      "seen            -> 0\n",
      "no              -> 0\n",
      "reason          -> 0\n",
      "to              -> 0\n",
      "abandon         -> 0\n",
      "it              -> 0\n",
      "when            -> 1\n",
      "she             -> 1\n",
      "came            -> 1\n",
      "to              -> 1\n",
      "med             -> 1\n",
      "##ew            -> 1\n",
      "##ich           -> 1\n",
      "two             -> 1\n",
      "years           -> 1\n",
      "ago             -> 1\n",
      ",               -> 0\n",
      "even            -> 0\n",
      "though          -> 0\n",
      "she             -> 0\n",
      "might           -> 0\n",
      "now             -> 0\n",
      "have            -> 0\n",
      "been            -> 0\n",
      "able            -> 0\n",
      "to              -> 0\n",
      "afford          -> 0\n",
      "a               -> 0\n",
      "car             -> 0\n",
      ".               -> 0\n",
      "[SEP]           -> 0\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n"
     ]
    }
   ],
   "source": [
    "labels = labels_tensor[sample_idx]\n",
    "for tok, label in zip(tokens, labels):\n",
    "    print(f\"{tok:15} -> {label.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "validation_split = 0.5\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Shuffle the data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation (without shuffling)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SubsetRandomSampler(range(len(val_dataset))),  # Don't shuffle validation data\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class FrameElementClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        #self.query_encoder = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.token_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    #def forward(self, input_ids, attention_mask, role_ids, role_mask):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode sentence\n",
    "        sentence_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = sentence_outputs.last_hidden_state  # shape: (B, T, H)\n",
    "        \"\"\"\n",
    "        # Encode role label (like \"Time\" or \"Manner\")\n",
    "        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\n",
    "        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\n",
    "        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\n",
    "\"\"\"\n",
    "        # Project sentence tokens\n",
    "        token_embeddings = self.token_projection(token_embeddings)  # shape: (B, T, H)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "        # Compute dot product between role embedding and each token\n",
    "        #role_embedding = role_embedding.unsqueeze(2)  # (B, H, 1)\n",
    "        #scores = torch.bmm(token_embeddings, role_embedding).squeeze(-1)  # shape: (B, T)\n",
    "\n",
    "        # Optionally apply attention mask\n",
    "        #scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        #probs = torch.softmax(logits, dim=-1)\n",
    "        #logits = self.classifier(token_embeddings)\n",
    "        return logits  # Apply softmax for inference or use with CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for a few more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "accuracies = []\n",
    "num_batches = 15\n",
    "model = FrameElementClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor([0.4, 0.6]).to(device)  # Make Time more important\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.9206 (313/340)\n",
      "Confusion Matrix (for 'Time' class prediction):\n",
      "[[8107 1153]\n",
      " [  27  313]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "# For confusion matrix\n",
    "all_true_binary = []\n",
    "all_pred_binary = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Binary labels: 1 for \"Time\", 0 for everything else\n",
    "        is_time_token = (target_index == 1)\n",
    "        predicted_time_token = (predicted_tokens == 1)\n",
    "\n",
    "        correct_time_preds = predicted_time_token & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "        # Flatten and convert to binary 0/1\n",
    "        all_true_binary.extend(is_time_token.view(-1).cpu().numpy())\n",
    "        all_pred_binary.extend(predicted_time_token.view(-1).cpu().numpy())\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_true_binary, all_pred_binary)\n",
    "print(\"Confusion Matrix (for 'Time' class prediction):\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(all_true_binary, all_pred_binary)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Time FE Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.8938 (261/292)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Only count tokens that were labeled as \"Time\" (class 1) in the ground truth\n",
    "        is_time_token = (target_index == 1)\n",
    "        correct_time_preds = (predicted_tokens == 1) & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "\n",
      "--- Example 2 ---\n",
      "Predicted : [0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 3 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "\n",
      "--- Example 4 ---\n",
      "Predicted : [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "True      : [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "\n",
      "--- Example 5 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_printed = 0\n",
    "max_to_print = 5  # Adjust how many examples you want to print\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches or num_printed >= max_to_print:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Loop through each sentence in the batch\n",
    "        for b in range(input_ids.size(0)):\n",
    "            mask = target_index[b] != -100\n",
    "            true_labels = target_index[b][mask].cpu().numpy()\n",
    "            pred_labels = predicted_tokens[b][mask].cpu().numpy()\n",
    "\n",
    "            print(f\"\\n--- Example {num_printed + 1} ---\")\n",
    "            print(\"Predicted :\", pred_labels)\n",
    "            print(\"True      :\", true_labels)\n",
    "\n",
    "            num_printed += 1\n",
    "            if num_printed >= max_to_print:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Sentence 1:\n",
      "   Text: [CLS] hall ' s landmark visit to iraq started on sunday evening when he arrived here by land from amman, jordan. [SEP]\n",
      "   Tokens: ['[CLS]', 'hall', \"'\", 's', 'landmark', 'visit', 'to', 'iraq', 'started', 'on', 'sunday', 'evening', 'when', 'he', 'arrived', 'here', 'by', 'land', 'from', 'amman', ',', 'jordan', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     hall            | True: 0 | Pred: 0 ‚úÖ\n",
      "     '               | True: 0 | Pred: 0 ‚úÖ\n",
      "     s               | True: 0 | Pred: 0 ‚úÖ\n",
      "     landmark        | True: 0 | Pred: 0 ‚úÖ\n",
      "     visit           | True: 0 | Pred: 0 ‚úÖ\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     iraq            | True: 0 | Pred: 0 ‚úÖ\n",
      "     started         | True: 0 | Pred: 0 ‚úÖ\n",
      "     on              | True: 1 | Pred: 1 ‚úÖ\n",
      "     sunday          | True: 1 | Pred: 1 ‚úÖ\n",
      "     evening         | True: 1 | Pred: 1 ‚úÖ\n",
      "     when            | True: 0 | Pred: 0 ‚úÖ\n",
      "     he              | True: 0 | Pred: 0 ‚úÖ\n",
      "     arrived         | True: 0 | Pred: 0 ‚úÖ\n",
      "     here            | True: 0 | Pred: 0 ‚úÖ\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     land            | True: 0 | Pred: 0 ‚úÖ\n",
      "     from            | True: 0 | Pred: 0 ‚úÖ\n",
      "     amman           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     jordan          | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 2:\n",
      "   Text: [CLS] for those who are not so energetic, the high peaks can be reached by cable cars and chair lifts from mid june onwards. [SEP]\n",
      "   Tokens: ['[CLS]', 'for', 'those', 'who', 'are', 'not', 'so', 'energetic', ',', 'the', 'high', 'peaks', 'can', 'be', 'reached', 'by', 'cable', 'cars', 'and', 'chair', 'lifts', 'from', 'mid', 'june', 'onwards', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     for             | True: 0 | Pred: 0 ‚úÖ\n",
      "     those           | True: 0 | Pred: 0 ‚úÖ\n",
      "     who             | True: 0 | Pred: 0 ‚úÖ\n",
      "     are             | True: 0 | Pred: 0 ‚úÖ\n",
      "     not             | True: 0 | Pred: 0 ‚úÖ\n",
      "     so              | True: 0 | Pred: 0 ‚úÖ\n",
      "     energetic       | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     high            | True: 0 | Pred: 0 ‚úÖ\n",
      "     peaks           | True: 0 | Pred: 0 ‚úÖ\n",
      "     can             | True: 0 | Pred: 0 ‚úÖ\n",
      "     be              | True: 0 | Pred: 0 ‚úÖ\n",
      "     reached         | True: 0 | Pred: 0 ‚úÖ\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     cable           | True: 0 | Pred: 0 ‚úÖ\n",
      "     cars            | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     chair           | True: 0 | Pred: 0 ‚úÖ\n",
      "     lifts           | True: 0 | Pred: 0 ‚úÖ\n",
      "     from            | True: 1 | Pred: 1 ‚úÖ\n",
      "     mid             | True: 1 | Pred: 1 ‚úÖ\n",
      "     june            | True: 1 | Pred: 1 ‚úÖ\n",
      "     onwards         | True: 1 | Pred: 1 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 3:\n",
      "   Text: [CLS] switching to the smaller barriers in 1987 has proved child ' s play for the sweetheart from chigwell ever since. [SEP]\n",
      "   Tokens: ['[CLS]', 'switching', 'to', 'the', 'smaller', 'barriers', 'in', '1987', 'has', 'proved', 'child', \"'\", 's', 'play', 'for', 'the', 'sweetheart', 'from', 'chi', '##g', '##well', 'ever', 'since', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     switching       | True: 0 | Pred: 0 ‚úÖ\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     smaller         | True: 0 | Pred: 0 ‚úÖ\n",
      "     barriers        | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 0 | Pred: 1 ‚ùå\n",
      "     1987            | True: 0 | Pred: 1 ‚ùå\n",
      "     has             | True: 0 | Pred: 0 ‚úÖ\n",
      "     proved          | True: 0 | Pred: 0 ‚úÖ\n",
      "     child           | True: 0 | Pred: 0 ‚úÖ\n",
      "     '               | True: 0 | Pred: 0 ‚úÖ\n",
      "     s               | True: 0 | Pred: 0 ‚úÖ\n",
      "     play            | True: 0 | Pred: 0 ‚úÖ\n",
      "     for             | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     sweetheart      | True: 0 | Pred: 0 ‚úÖ\n",
      "     from            | True: 0 | Pred: 0 ‚úÖ\n",
      "     chi             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##g             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##well          | True: 0 | Pred: 0 ‚úÖ\n",
      "     ever            | True: 1 | Pred: 0 ‚ùå\n",
      "     since           | True: 1 | Pred: 0 ‚ùå\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 4:\n",
      "   Text: [CLS] the pay bill for health service bosses has rocketed by nine hundred per cent in five years, it was revealed yesterday. [SEP]\n",
      "   Tokens: ['[CLS]', 'the', 'pay', 'bill', 'for', 'health', 'service', 'bosses', 'has', 'rocket', '##ed', 'by', 'nine', 'hundred', 'per', 'cent', 'in', 'five', 'years', ',', 'it', 'was', 'revealed', 'yesterday', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     pay             | True: 0 | Pred: 0 ‚úÖ\n",
      "     bill            | True: 0 | Pred: 0 ‚úÖ\n",
      "     for             | True: 0 | Pred: 0 ‚úÖ\n",
      "     health          | True: 0 | Pred: 0 ‚úÖ\n",
      "     service         | True: 0 | Pred: 0 ‚úÖ\n",
      "     bosses          | True: 0 | Pred: 0 ‚úÖ\n",
      "     has             | True: 0 | Pred: 0 ‚úÖ\n",
      "     rocket          | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ed            | True: 0 | Pred: 0 ‚úÖ\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     nine            | True: 0 | Pred: 0 ‚úÖ\n",
      "     hundred         | True: 0 | Pred: 0 ‚úÖ\n",
      "     per             | True: 0 | Pred: 0 ‚úÖ\n",
      "     cent            | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 1 | Pred: 1 ‚úÖ\n",
      "     five            | True: 1 | Pred: 1 ‚úÖ\n",
      "     years           | True: 1 | Pred: 1 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     it              | True: 0 | Pred: 0 ‚úÖ\n",
      "     was             | True: 0 | Pred: 0 ‚úÖ\n",
      "     revealed        | True: 0 | Pred: 0 ‚úÖ\n",
      "     yesterday       | True: 0 | Pred: 1 ‚ùå\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 5:\n",
      "   Text: [CLS] the u. s. currency initially advanced in asia after the japanese government unveiled a spending package aimed at cutting japan ' s huge trade surplus - - - a major cause of the dollar ' s weakness. [SEP]\n",
      "   Tokens: ['[CLS]', 'the', 'u', '.', 's', '.', 'currency', 'initially', 'advanced', 'in', 'asia', 'after', 'the', 'japanese', 'government', 'unveiled', 'a', 'spending', 'package', 'aimed', 'at', 'cutting', 'japan', \"'\", 's', 'huge', 'trade', 'surplus', '-', '-', '-', 'a', 'major', 'cause', 'of', 'the', 'dollar', \"'\", 's', 'weakness', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     u               | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     s               | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     currency        | True: 0 | Pred: 0 ‚úÖ\n",
      "     initially       | True: 1 | Pred: 0 ‚ùå\n",
      "     advanced        | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 0 | Pred: 0 ‚úÖ\n",
      "     asia            | True: 0 | Pred: 0 ‚úÖ\n",
      "     after           | True: 0 | Pred: 1 ‚ùå\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     japanese        | True: 0 | Pred: 1 ‚ùå\n",
      "     government      | True: 0 | Pred: 1 ‚ùå\n",
      "     unveiled        | True: 0 | Pred: 1 ‚ùå\n",
      "     a               | True: 0 | Pred: 1 ‚ùå\n",
      "     spending        | True: 0 | Pred: 1 ‚ùå\n",
      "     package         | True: 0 | Pred: 1 ‚ùå\n",
      "     aimed           | True: 0 | Pred: 1 ‚ùå\n",
      "     at              | True: 0 | Pred: 1 ‚ùå\n",
      "     cutting         | True: 0 | Pred: 1 ‚ùå\n",
      "     japan           | True: 0 | Pred: 1 ‚ùå\n",
      "     '               | True: 0 | Pred: 1 ‚ùå\n",
      "     s               | True: 0 | Pred: 1 ‚ùå\n",
      "     huge            | True: 0 | Pred: 1 ‚ùå\n",
      "     trade           | True: 0 | Pred: 1 ‚ùå\n",
      "     surplus         | True: 0 | Pred: 1 ‚ùå\n",
      "     -               | True: 0 | Pred: 1 ‚ùå\n",
      "     -               | True: 0 | Pred: 1 ‚ùå\n",
      "     -               | True: 0 | Pred: 1 ‚ùå\n",
      "     a               | True: 0 | Pred: 1 ‚ùå\n",
      "     major           | True: 0 | Pred: 0 ‚úÖ\n",
      "     cause           | True: 0 | Pred: 1 ‚ùå\n",
      "     of              | True: 0 | Pred: 1 ‚ùå\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     dollar          | True: 0 | Pred: 1 ‚ùå\n",
      "     '               | True: 0 | Pred: 1 ‚ùå\n",
      "     s               | True: 0 | Pred: 1 ‚ùå\n",
      "     weakness        | True: 0 | Pred: 1 ‚ùå\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dataloader):\n\u001b[32m      7\u001b[39m     input_ids, attention_mask, target_index = [item.to(device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     probs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     preds = torch.argmax(torch.softmax(probs, dim=-\u001b[32m1\u001b[39m), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_ids.size(\u001b[32m0\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mFrameElementClassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# Encode sentence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         sentence_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m         token_embeddings = sentence_outputs.last_hidden_state  \u001b[38;5;66;03m# shape: (B, T, H)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m        # Encode role label (like \"Time\" or \"Manner\")\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:254\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    639\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    554\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_examples_to_print = 5  # or however many you want\n",
    "examples_printed = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(torch.softmax(probs, dim=-1), dim=-1)\n",
    "\n",
    "        for j in range(input_ids.size(0)):\n",
    "            if examples_printed >= num_examples_to_print:\n",
    "                break\n",
    "\n",
    "            input_id = input_ids[j]\n",
    "            attention = attention_mask[j]\n",
    "            pred = preds[j]\n",
    "            label = target_index[j]\n",
    "\n",
    "            # Only consider real (non-padding) tokens\n",
    "            mask = (attention == 1) & (label != -100)\n",
    "            input_id = input_id[mask]\n",
    "            pred = pred[mask]\n",
    "            label = label[mask]\n",
    "\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "            sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "            print(f\"\\nüîπ Sentence {examples_printed + 1}:\")\n",
    "            print(f\"   Text: {sentence}\")\n",
    "            print(f\"   Tokens: {tokens}\")\n",
    "            print(f\"   True Labels:     {label.tolist()}\")\n",
    "            print(f\"   Predicted Labels:{pred.tolist()}\")\n",
    "\n",
    "            # Optional: highlight mismatches\n",
    "            print(\"   Comparison:\")\n",
    "            for tok, gold, guess in zip(tokens, label.tolist(), pred.tolist()):\n",
    "                status = \"‚úÖ\" if gold == guess else \"‚ùå\"\n",
    "                print(f\"     {tok:15} | True: {gold} | Pred: {guess} {status}\")\n",
    "\n",
    "            examples_printed += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_binary_spans(label_seq):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, val in enumerate(label_seq):\n",
    "        if val == 1 and start is None:\n",
    "            start = i\n",
    "        elif val == 0 and start is not None:\n",
    "            spans.append((start, i - 1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(label_seq) - 1))\n",
    "    return spans\n",
    "\n",
    "def evaluate_binary_predictions(true_labels_list, pred_labels_list):\n",
    "    strict_match = 0\n",
    "    partial_match = 0\n",
    "    total_spans = 0\n",
    "\n",
    "    for true_seq, pred_seq in zip(true_labels_list, pred_labels_list):\n",
    "        true_spans = extract_binary_spans(true_seq)\n",
    "        pred_spans = extract_binary_spans(pred_seq)\n",
    "        total_spans += len(true_spans)\n",
    "\n",
    "        for t_start, t_end in true_spans:\n",
    "            t_range = set(range(t_start, t_end + 1))\n",
    "            match_found = False\n",
    "            for p_start, p_end in pred_spans:\n",
    "                p_range = set(range(p_start, p_end + 1))\n",
    "                if t_range == p_range:\n",
    "                    strict_match += 1\n",
    "                    match_found = True\n",
    "                    break\n",
    "                elif t_range & p_range:\n",
    "                    match_found = True\n",
    "            if match_found:\n",
    "                partial_match += 1\n",
    "\n",
    "    return {\n",
    "        \"Total Time Elements\": total_spans,\n",
    "        \"Strict Matches\": strict_match,\n",
    "        \"Partial Matches\": partial_match,\n",
    "        \"Strict Accuracy\": strict_match / total_spans if total_spans > 0 else 0,\n",
    "        \"Partial Accuracy\": partial_match / total_spans if total_spans > 0 else 0\n",
    "    }\n",
    "\n",
    "# ‚¨áÔ∏è EVALUATION CODE\n",
    "def evaluate_model(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels_all = []\n",
    "    pred_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=-1)  # shape: (B, T)\n",
    "\n",
    "            for label_seq, pred_seq, mask in zip(labels, predictions, attention_mask):\n",
    "                # Remove padding (-100) and apply attention mask\n",
    "                true_seq = [label.item() for label, m in zip(label_seq, mask) if m == 1 and label != -100]\n",
    "                pred_seq = [pred.item() for pred, m in zip(pred_seq, mask) if m == 1]\n",
    "\n",
    "                true_labels_all.append(true_seq)\n",
    "                pred_labels_all.append(pred_seq[:len(true_seq)])  # Match lengths just in case\n",
    "\n",
    "    return evaluate_binary_predictions(true_labels_all, pred_labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 902/902 [03:07<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Results:\n",
      "Total Time Elements: 4613\n",
      "Strict Matches: 3378\n",
      "Partial Matches: 4125\n",
      "Strict Accuracy: 0.732\n",
      "Partial Accuracy: 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model, val_dataloader, device)\n",
    "print(\"üìä Evaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what data is more likely to produce an error?\n",
    "\n",
    "escribe motivation and problem\n",
    "describe framnet\n",
    "tell how you extract data\n",
    "tell model you build\n",
    "talk about results and interpretation\n",
    "diagrams, interpret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
