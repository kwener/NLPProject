{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPProject.ipynb  NLPProject2.ipynb\n",
      "Notebook metadata fixed! You can now commit to GitHub.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from google.colab import drive\n",
    "\n",
    "# Get the notebook's filename (usually matches the GitHub repo name)\n",
    "!ls *.ipynb\n",
    "notebook_name = \"NLPProject.ipynb\"  # ‚Üê Replace with your filename\n",
    "\n",
    "# Load and fix the notebook\n",
    "with open(notebook_name, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Option A: Remove widgets metadata completely (recommended)\n",
    "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Option B: Or add the missing state key\n",
    "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "#     nb['metadata']['widgets']['state'] = {}\n",
    "\n",
    "# Save the fixed version\n",
    "with open(notebook_name, 'w') as f:\n",
    "    json.dump(nb, f)\n",
    "\n",
    "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/kierstenwener/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Element: Time, Sample Sentences: 8170\n",
      "Frame Element: Manner, Sample Sentences: 7612\n",
      "Frame Element: Place, Sample Sentences: 7037\n",
      "Frame Element: Degree, Sample Sentences: 7012\n",
      "Frame Element: Means, Sample Sentences: 5045\n",
      "Frame Element: Explanation, Sample Sentences: 4539\n",
      "Frame Element: Depictive, Sample Sentences: 4091\n",
      "Frame Element: Purpose, Sample Sentences: 4091\n",
      "Frame Element: Circumstances, Sample Sentences: 3219\n",
      "Frame Element: Duration, Sample Sentences: 3120\n"
     ]
    }
   ],
   "source": [
    "frame_element_counts = {}\n",
    "#for each frame, loops through all frame elements\n",
    "for frame in fn.frames():\n",
    "    frame_name = frame.name\n",
    "\n",
    "    for fe_name, fe in frame.FE.items():\n",
    "\n",
    "        sample_sentences = frame.lexUnit\n",
    "        num_sentences = len(sample_sentences)\n",
    "\n",
    "        # Store the count of sentences for each frame element\n",
    "        if fe_name in frame_element_counts:\n",
    "            frame_element_counts[fe_name] += num_sentences  # Add the new count to the existing one\n",
    "        else:\n",
    "            frame_element_counts[fe_name] = num_sentences\n",
    "\n",
    "sorted_frame_elements = sorted(frame_element_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for fe_name, count in sorted_frame_elements[:10]:\n",
    "    print(f\"Frame Element: {fe_name}, Sample Sentences: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_with_time_ex = {}\n",
    "for f in fn.frames():\n",
    "    for x in f.FE:\n",
    "        if x == \"Time\":\n",
    "            frames_with_time_ex[f.name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, char_labels, offsets):\n",
    "    token_labels = []\n",
    "    for (token_start, token_end) in offsets:\n",
    "        # For special tokens like [CLS] and [SEP], offset is usually (0,0)\n",
    "        if token_start == token_end:\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # If any character in the token is marked as Time,\n",
    "            # decide on a label for the entire token.\n",
    "            token_tag = \"O\"\n",
    "            for pos in range(token_start, token_end):\n",
    "                if pos < len(char_labels) and char_labels[pos] != \"O\":\n",
    "                    token_tag = char_labels[pos]\n",
    "                    break\n",
    "            token_labels.append(token_tag)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kierstenwener/Desktop/NLPProject/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "Input IDs: torch.Size([9013, 128])\n",
      "Attention Masks: torch.Size([9013, 128])\n",
      "Labels: torch.Size([9013, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.corpus import framenet as fn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Map BIO tags to IDs\n",
    "label2id = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Find frames that include \"Time\" as a frame element\n",
    "\n",
    "for name, frame in frames_with_time_ex.items():\n",
    "    # Print the frame name for reference\n",
    "    for lu in frame.lexUnit.values():\n",
    "        #print(f\"\\nLexical Unit: {lu['name']}\")\n",
    "        lu_data = fn.lu(lu['ID'])\n",
    "        for ex in lu_data['exemplars']:\n",
    "            text = ex['text']\n",
    "            char_labels = [\"O\"] * len(text)\n",
    "            has_time_fe = False\n",
    "\n",
    "            for fe in ex['FE']:\n",
    "                for i in fe:\n",
    "                    if i[2] == \"Time\":\n",
    "                        start, end = i[0], i[1]\n",
    "                        if start < end:\n",
    "                            char_labels[start] = \"B-Time\"\n",
    "                            for i in range(start+1, end):\n",
    "                                char_labels[i] = \"I-Time\"\n",
    "                            has_time_fe = True\n",
    "            if not has_time_fe:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Tokenize\n",
    "            tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # Map character-level labels to token-level labels\n",
    "            token_labels = align_labels_with_tokens(text, char_labels, offsets)\n",
    "            label2id_binary = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 1}  # Map both B-Time and I-Time to 1\n",
    "            # Pad remaining labels with -100 where attention mask is 0 (i.e., padding tokens)\n",
    "\n",
    "\n",
    "            label_ids = [label2id_binary.get(lab, 0) for lab in token_labels]\n",
    "            label_ids = [\n",
    "                label if mask == 1 else -100 \n",
    "                for label, mask in zip(label_ids, attention_mask)\n",
    "            ]\n",
    "            # Store tensors\n",
    "            input_ids_list.append(torch.tensor(input_ids))\n",
    "            attention_masks_list.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(label_ids))\n",
    "\n",
    "# Final dataset tensors\n",
    "input_ids_tensor = torch.stack(input_ids_list)\n",
    "attention_masks_tensor = torch.stack(attention_masks_list)\n",
    "labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "print(\"Tensor shapes:\")\n",
    "print(\"Input IDs:\", input_ids_tensor.shape)\n",
    "print(\"Attention Masks:\", attention_masks_tensor.shape)\n",
    "print(\"Labels:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: when\n"
     ]
    }
   ],
   "source": [
    "indices = (labels_tensor == 1).nonzero(as_tuple=False)\n",
    "sample_idx, token_idx = indices[0].tolist()\n",
    "token_id = input_ids_tensor[sample_idx][token_idx]\n",
    "token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "print(f\"Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'she', 'had', 'seen', 'no', 'reason', 'to', 'abandon', 'it', 'when', 'she', 'came', 'to', 'med', '##ew', '##ich', 'two', 'years', 'ago', ',', 'even', 'though', 'she', 'might', 'now', 'have', 'been', 'able', 'to', 'afford', 'a', 'car', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor[sample_idx])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           -> 0\n",
      "she             -> 0\n",
      "had             -> 0\n",
      "seen            -> 0\n",
      "no              -> 0\n",
      "reason          -> 0\n",
      "to              -> 0\n",
      "abandon         -> 0\n",
      "it              -> 0\n",
      "when            -> 1\n",
      "she             -> 1\n",
      "came            -> 1\n",
      "to              -> 1\n",
      "med             -> 1\n",
      "##ew            -> 1\n",
      "##ich           -> 1\n",
      "two             -> 1\n",
      "years           -> 1\n",
      "ago             -> 1\n",
      ",               -> 0\n",
      "even            -> 0\n",
      "though          -> 0\n",
      "she             -> 0\n",
      "might           -> 0\n",
      "now             -> 0\n",
      "have            -> 0\n",
      "been            -> 0\n",
      "able            -> 0\n",
      "to              -> 0\n",
      "afford          -> 0\n",
      "a               -> 0\n",
      "car             -> 0\n",
      ".               -> 0\n",
      "[SEP]           -> 0\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n"
     ]
    }
   ],
   "source": [
    "labels = labels_tensor[sample_idx]\n",
    "for tok, label in zip(tokens, labels):\n",
    "    print(f\"{tok:15} -> {label.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "validation_split = 0.5\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Shuffle the data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation (without shuffling)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SubsetRandomSampler(range(len(val_dataset))),  # Don't shuffle validation data\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class FrameElementClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        #self.query_encoder = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.token_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    #def forward(self, input_ids, attention_mask, role_ids, role_mask):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode sentence\n",
    "        sentence_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = sentence_outputs.last_hidden_state  # shape: (B, T, H)\n",
    "        \"\"\"\n",
    "        # Encode role label (like \"Time\" or \"Manner\")\n",
    "        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\n",
    "        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\n",
    "        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\n",
    "\"\"\"\n",
    "        # Project sentence tokens\n",
    "        token_embeddings = self.token_projection(token_embeddings)  # shape: (B, T, H)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "        # Compute dot product between role embedding and each token\n",
    "        #role_embedding = role_embedding.unsqueeze(2)  # (B, H, 1)\n",
    "        #scores = torch.bmm(token_embeddings, role_embedding).squeeze(-1)  # shape: (B, T)\n",
    "\n",
    "        # Optionally apply attention mask\n",
    "        #scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        #probs = torch.softmax(logits, dim=-1)\n",
    "        #logits = self.classifier(token_embeddings)\n",
    "        return logits  # Apply softmax for inference or use with CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for a few more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "accuracies = []\n",
    "num_batches = 15\n",
    "model = FrameElementClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor([0.4, 0.6]).to(device)  # Make Time more important\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.6895 (211/306)\n",
      "Confusion Matrix (for 'Time' class prediction):\n",
      "[[8873  421]\n",
      " [  95  211]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "# For confusion matrix\n",
    "all_true_binary = []\n",
    "all_pred_binary = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Binary labels: 1 for \"Time\", 0 for everything else\n",
    "        is_time_token = (target_index == 1)\n",
    "        predicted_time_token = (predicted_tokens == 1)\n",
    "\n",
    "        correct_time_preds = predicted_time_token & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "        # Flatten and convert to binary 0/1\n",
    "        all_true_binary.extend(is_time_token.view(-1).cpu().numpy())\n",
    "        all_pred_binary.extend(predicted_time_token.view(-1).cpu().numpy())\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_true_binary, all_pred_binary)\n",
    "print(\"Confusion Matrix (for 'Time' class prediction):\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Time FE Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.8051 (219/272)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Only count tokens that were labeled as \"Time\" (class 1) in the ground truth\n",
    "        is_time_token = (target_index == 1)\n",
    "        correct_time_preds = (predicted_tokens == 1) & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Predicted : [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 2 ---\n",
      "Predicted : [0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 3 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0]\n",
      "\n",
      "--- Example 4 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 0 0]\n",
      "\n",
      "--- Example 5 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_printed = 0\n",
    "max_to_print = 5  # Adjust how many examples you want to print\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches or num_printed >= max_to_print:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Loop through each sentence in the batch\n",
    "        for b in range(input_ids.size(0)):\n",
    "            mask = target_index[b] != -100\n",
    "            true_labels = target_index[b][mask].cpu().numpy()\n",
    "            pred_labels = predicted_tokens[b][mask].cpu().numpy()\n",
    "\n",
    "            print(f\"\\n--- Example {num_printed + 1} ---\")\n",
    "            print(\"Predicted :\", pred_labels)\n",
    "            print(\"True      :\", true_labels)\n",
    "\n",
    "            num_printed += 1\n",
    "            if num_printed >= max_to_print:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Sentence 1:\n",
      "   Text: [CLS] gloucestershire police confirmed today that they have questioned the pair. [SEP]\n",
      "   Tokens: ['[CLS]', 'gloucestershire', 'police', 'confirmed', 'today', 'that', 'they', 'have', 'questioned', 'the', 'pair', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     gloucestershire | True: 0 | Pred: 0 ‚úÖ\n",
      "     police          | True: 0 | Pred: 0 ‚úÖ\n",
      "     confirmed       | True: 0 | Pred: 0 ‚úÖ\n",
      "     today           | True: 1 | Pred: 1 ‚úÖ\n",
      "     that            | True: 0 | Pred: 0 ‚úÖ\n",
      "     they            | True: 0 | Pred: 0 ‚úÖ\n",
      "     have            | True: 0 | Pred: 0 ‚úÖ\n",
      "     questioned      | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     pair            | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 2:\n",
      "   Text: [CLS] two swedish engineers kidnapped by the moslem janbaz \" crusader \" force ( mjf ) in march 1991 [ see p. 38151 ] escaped from their captors on july 5. [SEP]\n",
      "   Tokens: ['[CLS]', 'two', 'swedish', 'engineers', 'kidnapped', 'by', 'the', 'mo', '##sle', '##m', 'jan', '##ba', '##z', '\"', 'crusader', '\"', 'force', '(', 'm', '##j', '##f', ')', 'in', 'march', '1991', '[', 'see', 'p', '.', '381', '##51', ']', 'escaped', 'from', 'their', 'capt', '##ors', 'on', 'july', '5', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     two             | True: 0 | Pred: 0 ‚úÖ\n",
      "     swedish         | True: 0 | Pred: 0 ‚úÖ\n",
      "     engineers       | True: 0 | Pred: 0 ‚úÖ\n",
      "     kidnapped       | True: 0 | Pred: 0 ‚úÖ\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     mo              | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##sle           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##m             | True: 0 | Pred: 0 ‚úÖ\n",
      "     jan             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ba            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##z             | True: 0 | Pred: 0 ‚úÖ\n",
      "     \"               | True: 0 | Pred: 0 ‚úÖ\n",
      "     crusader        | True: 0 | Pred: 0 ‚úÖ\n",
      "     \"               | True: 0 | Pred: 0 ‚úÖ\n",
      "     force           | True: 0 | Pred: 0 ‚úÖ\n",
      "     (               | True: 0 | Pred: 0 ‚úÖ\n",
      "     m               | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##j             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##f             | True: 0 | Pred: 0 ‚úÖ\n",
      "     )               | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 1 | Pred: 1 ‚úÖ\n",
      "     march           | True: 1 | Pred: 1 ‚úÖ\n",
      "     1991            | True: 1 | Pred: 1 ‚úÖ\n",
      "     [               | True: 0 | Pred: 0 ‚úÖ\n",
      "     see             | True: 0 | Pred: 0 ‚úÖ\n",
      "     p               | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     381             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##51            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ]               | True: 0 | Pred: 0 ‚úÖ\n",
      "     escaped         | True: 0 | Pred: 0 ‚úÖ\n",
      "     from            | True: 0 | Pred: 0 ‚úÖ\n",
      "     their           | True: 0 | Pred: 0 ‚úÖ\n",
      "     capt            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ors           | True: 0 | Pred: 0 ‚úÖ\n",
      "     on              | True: 0 | Pred: 1 ‚ùå\n",
      "     july            | True: 0 | Pred: 1 ‚ùå\n",
      "     5               | True: 0 | Pred: 1 ‚ùå\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 3:\n",
      "   Text: [CLS] ` ye ' ll see what ah mean when ye get to the bathroom. [SEP]\n",
      "   Tokens: ['[CLS]', '`', 'ye', \"'\", 'll', 'see', 'what', 'ah', 'mean', 'when', 'ye', 'get', 'to', 'the', 'bathroom', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     `               | True: 0 | Pred: 0 ‚úÖ\n",
      "     ye              | True: 0 | Pred: 0 ‚úÖ\n",
      "     '               | True: 0 | Pred: 0 ‚úÖ\n",
      "     ll              | True: 0 | Pred: 0 ‚úÖ\n",
      "     see             | True: 0 | Pred: 0 ‚úÖ\n",
      "     what            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ah              | True: 0 | Pred: 0 ‚úÖ\n",
      "     mean            | True: 0 | Pred: 0 ‚úÖ\n",
      "     when            | True: 1 | Pred: 1 ‚úÖ\n",
      "     ye              | True: 1 | Pred: 1 ‚úÖ\n",
      "     get             | True: 1 | Pred: 1 ‚úÖ\n",
      "     to              | True: 1 | Pred: 1 ‚úÖ\n",
      "     the             | True: 1 | Pred: 1 ‚úÖ\n",
      "     bathroom        | True: 1 | Pred: 1 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 4:\n",
      "   Text: [CLS] the seasonal construction job he obtained soon ended, and he returned to career academy for help once more. [SEP]\n",
      "   Tokens: ['[CLS]', 'the', 'seasonal', 'construction', 'job', 'he', 'obtained', 'soon', 'ended', ',', 'and', 'he', 'returned', 'to', 'career', 'academy', 'for', 'help', 'once', 'more', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     seasonal        | True: 0 | Pred: 0 ‚úÖ\n",
      "     construction    | True: 0 | Pred: 0 ‚úÖ\n",
      "     job             | True: 0 | Pred: 0 ‚úÖ\n",
      "     he              | True: 0 | Pred: 0 ‚úÖ\n",
      "     obtained        | True: 0 | Pred: 0 ‚úÖ\n",
      "     soon            | True: 1 | Pred: 1 ‚úÖ\n",
      "     ended           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     he              | True: 0 | Pred: 0 ‚úÖ\n",
      "     returned        | True: 0 | Pred: 0 ‚úÖ\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     career          | True: 0 | Pred: 0 ‚úÖ\n",
      "     academy         | True: 0 | Pred: 0 ‚úÖ\n",
      "     for             | True: 0 | Pred: 0 ‚úÖ\n",
      "     help            | True: 0 | Pred: 0 ‚úÖ\n",
      "     once            | True: 0 | Pred: 0 ‚úÖ\n",
      "     more            | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 5:\n",
      "   Text: [CLS] if the computer encounters def during execution of the program, the rest of the line is ignored. [SEP]\n",
      "   Tokens: ['[CLS]', 'if', 'the', 'computer', 'encounters', 'def', 'during', 'execution', 'of', 'the', 'program', ',', 'the', 'rest', 'of', 'the', 'line', 'is', 'ignored', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     if              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     computer        | True: 0 | Pred: 0 ‚úÖ\n",
      "     encounters      | True: 0 | Pred: 0 ‚úÖ\n",
      "     def             | True: 0 | Pred: 0 ‚úÖ\n",
      "     during          | True: 1 | Pred: 1 ‚úÖ\n",
      "     execution       | True: 1 | Pred: 1 ‚úÖ\n",
      "     of              | True: 1 | Pred: 1 ‚úÖ\n",
      "     the             | True: 1 | Pred: 1 ‚úÖ\n",
      "     program         | True: 1 | Pred: 1 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     rest            | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     line            | True: 0 | Pred: 0 ‚úÖ\n",
      "     is              | True: 0 | Pred: 0 ‚úÖ\n",
      "     ignored         | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dataloader):\n\u001b[32m      7\u001b[39m     input_ids, attention_mask, target_index = [item.to(device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     probs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     preds = torch.argmax(torch.softmax(probs, dim=-\u001b[32m1\u001b[39m), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_ids.size(\u001b[32m0\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mFrameElementClassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# Encode sentence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         sentence_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m         token_embeddings = sentence_outputs.last_hidden_state  \u001b[38;5;66;03m# shape: (B, T, H)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m        # Encode role label (like \"Time\" or \"Manner\")\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:254\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_examples_to_print = 5  # or however many you want\n",
    "examples_printed = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(torch.softmax(probs, dim=-1), dim=-1)\n",
    "\n",
    "        for j in range(input_ids.size(0)):\n",
    "            if examples_printed >= num_examples_to_print:\n",
    "                break\n",
    "\n",
    "            input_id = input_ids[j]\n",
    "            attention = attention_mask[j]\n",
    "            pred = preds[j]\n",
    "            label = target_index[j]\n",
    "\n",
    "            # Only consider real (non-padding) tokens\n",
    "            mask = (attention == 1) & (label != -100)\n",
    "            input_id = input_id[mask]\n",
    "            pred = pred[mask]\n",
    "            label = label[mask]\n",
    "\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "            sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "            print(f\"\\nüîπ Sentence {examples_printed + 1}:\")\n",
    "            print(f\"   Text: {sentence}\")\n",
    "            print(f\"   Tokens: {tokens}\")\n",
    "            print(f\"   True Labels:     {label.tolist()}\")\n",
    "            print(f\"   Predicted Labels:{pred.tolist()}\")\n",
    "\n",
    "            # Optional: highlight mismatches\n",
    "            print(\"   Comparison:\")\n",
    "            for tok, gold, guess in zip(tokens, label.tolist(), pred.tolist()):\n",
    "                status = \"‚úÖ\" if gold == guess else \"‚ùå\"\n",
    "                print(f\"     {tok:15} | True: {gold} | Pred: {guess} {status}\")\n",
    "\n",
    "            examples_printed += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_binary_spans(label_seq):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, val in enumerate(label_seq):\n",
    "        if val == 1 and start is None:\n",
    "            start = i\n",
    "        elif val == 0 and start is not None:\n",
    "            spans.append((start, i - 1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(label_seq) - 1))\n",
    "    return spans\n",
    "\n",
    "def evaluate_binary_predictions(true_labels_list, pred_labels_list):\n",
    "    strict_match = 0\n",
    "    partial_match = 0\n",
    "    total_spans = 0\n",
    "\n",
    "    for true_seq, pred_seq in zip(true_labels_list, pred_labels_list):\n",
    "        true_spans = extract_binary_spans(true_seq)\n",
    "        pred_spans = extract_binary_spans(pred_seq)\n",
    "        total_spans += len(true_spans)\n",
    "\n",
    "        for t_start, t_end in true_spans:\n",
    "            t_range = set(range(t_start, t_end + 1))\n",
    "            match_found = False\n",
    "            for p_start, p_end in pred_spans:\n",
    "                p_range = set(range(p_start, p_end + 1))\n",
    "                if t_range == p_range:\n",
    "                    strict_match += 1\n",
    "                    match_found = True\n",
    "                    break\n",
    "                elif t_range & p_range:\n",
    "                    match_found = True\n",
    "            if match_found:\n",
    "                partial_match += 1\n",
    "\n",
    "    return {\n",
    "        \"Total Time Elements\": total_spans,\n",
    "        \"Strict Matches\": strict_match,\n",
    "        \"Partial Matches\": partial_match,\n",
    "        \"Strict Accuracy\": strict_match / total_spans if total_spans > 0 else 0,\n",
    "        \"Partial Accuracy\": partial_match / total_spans if total_spans > 0 else 0\n",
    "    }\n",
    "\n",
    "# ‚¨áÔ∏è EVALUATION CODE\n",
    "def evaluate_model(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels_all = []\n",
    "    pred_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=-1)  # shape: (B, T)\n",
    "\n",
    "            for label_seq, pred_seq, mask in zip(labels, predictions, attention_mask):\n",
    "                # Remove padding (-100) and apply attention mask\n",
    "                true_seq = [label.item() for label, m in zip(label_seq, mask) if m == 1 and label != -100]\n",
    "                pred_seq = [pred.item() for pred, m in zip(pred_seq, mask) if m == 1]\n",
    "\n",
    "                true_labels_all.append(true_seq)\n",
    "                pred_labels_all.append(pred_seq[:len(true_seq)])  # Match lengths just in case\n",
    "\n",
    "    return evaluate_binary_predictions(true_labels_all, pred_labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 902/902 [03:14<00:00,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Results:\n",
      "Total Time Elements: 4612\n",
      "Strict Matches: 3393\n",
      "Partial Matches: 4221\n",
      "Strict Accuracy: 0.736\n",
      "Partial Accuracy: 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model, val_dataloader, device)\n",
    "print(\"üìä Evaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
