{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPProject.ipynb  NLPProject2.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook metadata fixed! You can now commit to GitHub.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from google.colab import drive\n",
    "\n",
    "# Get the notebook's filename (usually matches the GitHub repo name)\n",
    "!ls *.ipynb\n",
    "notebook_name = \"NLPProject.ipynb\"  # ‚Üê Replace with your filename\n",
    "\n",
    "# Load and fix the notebook\n",
    "with open(notebook_name, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Option A: Remove widgets metadata completely (recommended)\n",
    "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Option B: Or add the missing state key\n",
    "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "#     nb['metadata']['widgets']['state'] = {}\n",
    "\n",
    "# Save the fixed version\n",
    "with open(notebook_name, 'w') as f:\n",
    "    json.dump(nb, f)\n",
    "\n",
    "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/kierstenwener/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Element: Time, Sample Sentences: 8170\n",
      "Frame Element: Manner, Sample Sentences: 7612\n",
      "Frame Element: Place, Sample Sentences: 7037\n",
      "Frame Element: Degree, Sample Sentences: 7012\n",
      "Frame Element: Means, Sample Sentences: 5045\n",
      "Frame Element: Explanation, Sample Sentences: 4539\n",
      "Frame Element: Depictive, Sample Sentences: 4091\n",
      "Frame Element: Purpose, Sample Sentences: 4091\n",
      "Frame Element: Circumstances, Sample Sentences: 3219\n",
      "Frame Element: Duration, Sample Sentences: 3120\n"
     ]
    }
   ],
   "source": [
    "frame_element_counts = {}\n",
    "#for each frame, loops through all frame elements\n",
    "for frame in fn.frames():\n",
    "    frame_name = frame.name\n",
    "\n",
    "    for fe_name, fe in frame.FE.items():\n",
    "\n",
    "        sample_sentences = frame.lexUnit\n",
    "        num_sentences = len(sample_sentences)\n",
    "\n",
    "        # Store the count of sentences for each frame element\n",
    "        if fe_name in frame_element_counts:\n",
    "            frame_element_counts[fe_name] += num_sentences  # Add the new count to the existing one\n",
    "        else:\n",
    "            frame_element_counts[fe_name] = num_sentences\n",
    "\n",
    "sorted_frame_elements = sorted(frame_element_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for fe_name, count in sorted_frame_elements[:10]:\n",
    "    print(f\"Frame Element: {fe_name}, Sample Sentences: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_with_time_ex = {}\n",
    "for f in fn.frames():\n",
    "    for x in f.FE:\n",
    "        if x == \"Time\":\n",
    "            frames_with_time_ex[f.name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, char_labels, offsets):\n",
    "    token_labels = []\n",
    "    for (token_start, token_end) in offsets:\n",
    "        # For special tokens like [CLS] and [SEP], offset is usually (0,0)\n",
    "        if token_start == token_end:\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # If any character in the token is marked as Time,\n",
    "            # decide on a label for the entire token.\n",
    "            token_tag = \"O\"\n",
    "            for pos in range(token_start, token_end):\n",
    "                if pos < len(char_labels) and char_labels[pos] != \"O\":\n",
    "                    token_tag = char_labels[pos]\n",
    "                    break\n",
    "            token_labels.append(token_tag)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "Input IDs: torch.Size([9013, 128])\n",
      "Attention Masks: torch.Size([9013, 128])\n",
      "Labels: torch.Size([9013, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.corpus import framenet as fn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Map BIO tags to IDs\n",
    "label2id = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Find frames that include \"Time\" as a frame element\n",
    "\n",
    "for name, frame in frames_with_time_ex.items():\n",
    "    # Print the frame name for reference\n",
    "    for lu in frame.lexUnit.values():\n",
    "        #print(f\"\\nLexical Unit: {lu['name']}\")\n",
    "        lu_data = fn.lu(lu['ID'])\n",
    "        for ex in lu_data['exemplars']:\n",
    "            text = ex['text']\n",
    "            char_labels = [\"O\"] * len(text)\n",
    "            has_time_fe = False\n",
    "\n",
    "            for fe in ex['FE']:\n",
    "                for i in fe:\n",
    "                    if i[2] == \"Time\":\n",
    "                        start, end = i[0], i[1]\n",
    "                        if start < end:\n",
    "                            char_labels[start] = \"B-Time\"\n",
    "                            for i in range(start+1, end):\n",
    "                                char_labels[i] = \"I-Time\"\n",
    "                            has_time_fe = True\n",
    "            if not has_time_fe:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Tokenize\n",
    "            tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # Map character-level labels to token-level labels\n",
    "            token_labels = align_labels_with_tokens(text, char_labels, offsets)\n",
    "            label2id_binary = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 1}  # Map both B-Time and I-Time to 1\n",
    "            # Pad remaining labels with -100 where attention mask is 0 (i.e., padding tokens)\n",
    "\n",
    "\n",
    "            label_ids = [label2id_binary.get(lab, 0) for lab in token_labels]\n",
    "            label_ids = [\n",
    "                label if mask == 1 else -100 \n",
    "                for label, mask in zip(label_ids, attention_mask)\n",
    "            ]\n",
    "            # Store tensors\n",
    "            input_ids_list.append(torch.tensor(input_ids))\n",
    "            attention_masks_list.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(label_ids))\n",
    "\n",
    "# Final dataset tensors\n",
    "input_ids_tensor = torch.stack(input_ids_list)\n",
    "attention_masks_tensor = torch.stack(attention_masks_list)\n",
    "labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "print(\"Tensor shapes:\")\n",
    "print(\"Input IDs:\", input_ids_tensor.shape)\n",
    "print(\"Attention Masks:\", attention_masks_tensor.shape)\n",
    "print(\"Labels:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: when\n"
     ]
    }
   ],
   "source": [
    "indices = (labels_tensor == 1).nonzero(as_tuple=False)\n",
    "sample_idx, token_idx = indices[0].tolist()\n",
    "token_id = input_ids_tensor[sample_idx][token_idx]\n",
    "token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "print(f\"Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'she', 'had', 'seen', 'no', 'reason', 'to', 'abandon', 'it', 'when', 'she', 'came', 'to', 'med', '##ew', '##ich', 'two', 'years', 'ago', ',', 'even', 'though', 'she', 'might', 'now', 'have', 'been', 'able', 'to', 'afford', 'a', 'car', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor[sample_idx])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           -> 0\n",
      "she             -> 0\n",
      "had             -> 0\n",
      "seen            -> 0\n",
      "no              -> 0\n",
      "reason          -> 0\n",
      "to              -> 0\n",
      "abandon         -> 0\n",
      "it              -> 0\n",
      "when            -> 1\n",
      "she             -> 1\n",
      "came            -> 1\n",
      "to              -> 1\n",
      "med             -> 1\n",
      "##ew            -> 1\n",
      "##ich           -> 1\n",
      "two             -> 1\n",
      "years           -> 1\n",
      "ago             -> 1\n",
      ",               -> 0\n",
      "even            -> 0\n",
      "though          -> 0\n",
      "she             -> 0\n",
      "might           -> 0\n",
      "now             -> 0\n",
      "have            -> 0\n",
      "been            -> 0\n",
      "able            -> 0\n",
      "to              -> 0\n",
      "afford          -> 0\n",
      "a               -> 0\n",
      "car             -> 0\n",
      ".               -> 0\n",
      "[SEP]           -> 0\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n"
     ]
    }
   ],
   "source": [
    "labels = labels_tensor[sample_idx]\n",
    "for tok, label in zip(tokens, labels):\n",
    "    print(f\"{tok:15} -> {label.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "validation_split = 0.5\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Shuffle the data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation (without shuffling)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SubsetRandomSampler(range(len(val_dataset))),  # Don't shuffle validation data\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class FrameElementClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        #self.query_encoder = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.token_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    #def forward(self, input_ids, attention_mask, role_ids, role_mask):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode sentence\n",
    "        sentence_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = sentence_outputs.last_hidden_state  # shape: (B, T, H)\n",
    "        \"\"\"\n",
    "        # Encode role label (like \"Time\" or \"Manner\")\n",
    "        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\n",
    "        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\n",
    "        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\n",
    "\"\"\"\n",
    "        # Project sentence tokens\n",
    "        token_embeddings = self.token_projection(token_embeddings)  # shape: (B, T, H)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "        # Compute dot product between role embedding and each token\n",
    "        #role_embedding = role_embedding.unsqueeze(2)  # (B, H, 1)\n",
    "        #scores = torch.bmm(token_embeddings, role_embedding).squeeze(-1)  # shape: (B, T)\n",
    "\n",
    "        # Optionally apply attention mask\n",
    "        #scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        #probs = torch.softmax(logits, dim=-1)\n",
    "        #logits = self.classifier(token_embeddings)\n",
    "        return logits  # Apply softmax for inference or use with CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 1, Validation Accuracy: 0.6668\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 2, Validation Accuracy: 0.6681\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 3, Validation Accuracy: 0.6600\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 4, Validation Accuracy: 0.6692\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 5, Validation Accuracy: 0.6709\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "accuracies = []\n",
    "num_batches = 100\n",
    "model = FrameElementClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor([0.2, 0.8]).to(device)  # Make Time more important\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_positives = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "            probs = model(input_ids, attention_mask)\n",
    "            probs_softmax = torch.softmax(probs, dim=-1)\n",
    "            predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "            all_zeros_predicted = (predicted_tokens == 0).all()\n",
    "            mask = target_index != -100\n",
    "            #print(\"All predicted tokens are 0:\", all_zeros_predicted.item())      \n",
    "            #all_zeros_true = (target_index == 0).all()\n",
    "            #print(\"All true tokens are 0:\", all_zeros_true.item()) \n",
    "            correct += ((predicted_tokens == target_index) & mask).sum().item()\n",
    "            true_positives += ((predicted_tokens == 1) & (target_index == 1)).sum().item()\n",
    "            if true_positives > 0: print(\"Found a trie positive\")\n",
    "            total += mask.sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for a few more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.1920 (431/2245)\n",
      "Confusion Matrix (for 'Time' class prediction):\n",
      "[[48701 13054]\n",
      " [ 1814   431]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "# For confusion matrix\n",
    "all_true_binary = []\n",
    "all_pred_binary = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Binary labels: 1 for \"Time\", 0 for everything else\n",
    "        is_time_token = (target_index == 1)\n",
    "        predicted_time_token = (predicted_tokens == 1)\n",
    "\n",
    "        correct_time_preds = predicted_time_token & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "        # Flatten and convert to binary 0/1\n",
    "        all_true_binary.extend(is_time_token.view(-1).cpu().numpy())\n",
    "        all_pred_binary.extend(predicted_time_token.view(-1).cpu().numpy())\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_true_binary, all_pred_binary)\n",
    "print(\"Confusion Matrix (for 'Time' class prediction):\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Time FE Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.1776 (364/2049)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Only count tokens that were labeled as \"Time\" (class 1) in the ground truth\n",
    "        is_time_token = (target_index == 1)\n",
    "        correct_time_preds = (predicted_tokens == 1) & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 2 ---\n",
      "Predicted : [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 3 ---\n",
      "Predicted : [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 1]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0]\n",
      "\n",
      "--- Example 4 ---\n",
      "Predicted : [0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      "True      : [0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 5 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      "True      : [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_printed = 0\n",
    "max_to_print = 5  # Adjust how many examples you want to print\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches or num_printed >= max_to_print:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Loop through each sentence in the batch\n",
    "        for b in range(input_ids.size(0)):\n",
    "            mask = target_index[b] != -100\n",
    "            true_labels = target_index[b][mask].cpu().numpy()\n",
    "            pred_labels = predicted_tokens[b][mask].cpu().numpy()\n",
    "\n",
    "            print(f\"\\n--- Example {num_printed + 1} ---\")\n",
    "            print(\"Predicted :\", pred_labels)\n",
    "            print(\"True      :\", true_labels)\n",
    "\n",
    "            num_printed += 1\n",
    "            if num_printed >= max_to_print:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Sentence 1:\n",
      "   Text: [CLS] the three main services at dawn, midday and dusk comprised the washing, anointment, adornment with clothing and regalia, and feeding of the deity with offerings. [SEP]\n",
      "   Tokens: ['[CLS]', 'the', 'three', 'main', 'services', 'at', 'dawn', ',', 'midday', 'and', 'dusk', 'comprised', 'the', 'washing', ',', 'an', '##oint', '##ment', ',', 'ad', '##orn', '##ment', 'with', 'clothing', 'and', 'regal', '##ia', ',', 'and', 'feeding', 'of', 'the', 'deity', 'with', 'offerings', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     three           | True: 0 | Pred: 0 ‚úÖ\n",
      "     main            | True: 0 | Pred: 0 ‚úÖ\n",
      "     services        | True: 0 | Pred: 0 ‚úÖ\n",
      "     at              | True: 1 | Pred: 0 ‚ùå\n",
      "     dawn            | True: 1 | Pred: 0 ‚ùå\n",
      "     ,               | True: 1 | Pred: 0 ‚ùå\n",
      "     midday          | True: 1 | Pred: 0 ‚ùå\n",
      "     and             | True: 1 | Pred: 0 ‚ùå\n",
      "     dusk            | True: 1 | Pred: 0 ‚ùå\n",
      "     comprised       | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     washing         | True: 0 | Pred: 1 ‚ùå\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     an              | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##oint          | True: 0 | Pred: 1 ‚ùå\n",
      "     ##ment          | True: 0 | Pred: 1 ‚ùå\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     ad              | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##orn           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ment          | True: 0 | Pred: 1 ‚ùå\n",
      "     with            | True: 0 | Pred: 0 ‚úÖ\n",
      "     clothing        | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     regal           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ia            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 1 ‚ùå\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     feeding         | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     deity           | True: 0 | Pred: 1 ‚ùå\n",
      "     with            | True: 0 | Pred: 0 ‚úÖ\n",
      "     offerings       | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 2:\n",
      "   Text: [CLS] this practice was also advocated by hippocrates, the ` father of medicine \", several centuries before. [SEP]\n",
      "   Tokens: ['[CLS]', 'this', 'practice', 'was', 'also', 'advocated', 'by', 'hip', '##po', '##crat', '##es', ',', 'the', '`', 'father', 'of', 'medicine', '\"', ',', 'several', 'centuries', 'before', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     this            | True: 0 | Pred: 0 ‚úÖ\n",
      "     practice        | True: 0 | Pred: 0 ‚úÖ\n",
      "     was             | True: 0 | Pred: 0 ‚úÖ\n",
      "     also            | True: 0 | Pred: 0 ‚úÖ\n",
      "     advocated       | True: 0 | Pred: 0 ‚úÖ\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     hip             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##po            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##crat          | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##es            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     `               | True: 0 | Pred: 0 ‚úÖ\n",
      "     father          | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     medicine        | True: 0 | Pred: 0 ‚úÖ\n",
      "     \"               | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 1 ‚ùå\n",
      "     several         | True: 1 | Pred: 0 ‚ùå\n",
      "     centuries       | True: 1 | Pred: 0 ‚ùå\n",
      "     before          | True: 1 | Pred: 0 ‚ùå\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 1 ‚ùå\n",
      "\n",
      "üîπ Sentence 3:\n",
      "   Text: [CLS] alan grant right, and one of them was killed by the spinosaur while i was on the island. [SEP]\n",
      "   Tokens: ['[CLS]', 'alan', 'grant', 'right', ',', 'and', 'one', 'of', 'them', 'was', 'killed', 'by', 'the', 'spin', '##osa', '##ur', 'while', 'i', 'was', 'on', 'the', 'island', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     alan            | True: 0 | Pred: 0 ‚úÖ\n",
      "     grant           | True: 0 | Pred: 0 ‚úÖ\n",
      "     right           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     one             | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     them            | True: 0 | Pred: 0 ‚úÖ\n",
      "     was             | True: 0 | Pred: 0 ‚úÖ\n",
      "     killed          | True: 0 | Pred: 0 ‚úÖ\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     spin            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##osa           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ur            | True: 0 | Pred: 0 ‚úÖ\n",
      "     while           | True: 1 | Pred: 0 ‚ùå\n",
      "     i               | True: 1 | Pred: 0 ‚ùå\n",
      "     was             | True: 1 | Pred: 0 ‚ùå\n",
      "     on              | True: 1 | Pred: 0 ‚ùå\n",
      "     the             | True: 1 | Pred: 0 ‚ùå\n",
      "     island          | True: 1 | Pred: 0 ‚ùå\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 4:\n",
      "   Text: [CLS] he needs to keep his finger on the pulse to succeed during the short tourist season. [SEP]\n",
      "   Tokens: ['[CLS]', 'he', 'needs', 'to', 'keep', 'his', 'finger', 'on', 'the', 'pulse', 'to', 'succeed', 'during', 'the', 'short', 'tourist', 'season', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     he              | True: 0 | Pred: 1 ‚ùå\n",
      "     needs           | True: 0 | Pred: 1 ‚ùå\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     keep            | True: 0 | Pred: 0 ‚úÖ\n",
      "     his             | True: 0 | Pred: 0 ‚úÖ\n",
      "     finger          | True: 0 | Pred: 0 ‚úÖ\n",
      "     on              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     pulse           | True: 0 | Pred: 0 ‚úÖ\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     succeed         | True: 0 | Pred: 0 ‚úÖ\n",
      "     during          | True: 1 | Pred: 0 ‚ùå\n",
      "     the             | True: 1 | Pred: 0 ‚ùå\n",
      "     short           | True: 1 | Pred: 0 ‚ùå\n",
      "     tourist         | True: 1 | Pred: 0 ‚ùå\n",
      "     season          | True: 1 | Pred: 0 ‚ùå\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 1 ‚ùå\n",
      "\n",
      "üîπ Sentence 5:\n",
      "   Text: [CLS] in that case the conviction of three youths in 1972 was later overturned by the court of appeal in a judgment which by implication raised disturbing questions about police interrogations, especially of juveniles and the mentally handicapped. [SEP]\n",
      "   Tokens: ['[CLS]', 'in', 'that', 'case', 'the', 'conviction', 'of', 'three', 'youths', 'in', '1972', 'was', 'later', 'overturned', 'by', 'the', 'court', 'of', 'appeal', 'in', 'a', 'judgment', 'which', 'by', 'implication', 'raised', 'disturbing', 'questions', 'about', 'police', 'interrogation', '##s', ',', 'especially', 'of', 'juveniles', 'and', 'the', 'mentally', 'handicapped', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 0 | Pred: 0 ‚úÖ\n",
      "     that            | True: 0 | Pred: 1 ‚ùå\n",
      "     case            | True: 0 | Pred: 1 ‚ùå\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     conviction      | True: 0 | Pred: 1 ‚ùå\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     three           | True: 0 | Pred: 0 ‚úÖ\n",
      "     youths          | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 1 | Pred: 0 ‚ùå\n",
      "     1972            | True: 1 | Pred: 0 ‚ùå\n",
      "     was             | True: 0 | Pred: 0 ‚úÖ\n",
      "     later           | True: 0 | Pred: 0 ‚úÖ\n",
      "     overturned      | True: 0 | Pred: 1 ‚ùå\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     court           | True: 0 | Pred: 1 ‚ùå\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     appeal          | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 0 | Pred: 0 ‚úÖ\n",
      "     a               | True: 0 | Pred: 0 ‚úÖ\n",
      "     judgment        | True: 0 | Pred: 1 ‚ùå\n",
      "     which           | True: 0 | Pred: 0 ‚úÖ\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     implication     | True: 0 | Pred: 0 ‚úÖ\n",
      "     raised          | True: 0 | Pred: 1 ‚ùå\n",
      "     disturbing      | True: 0 | Pred: 0 ‚úÖ\n",
      "     questions       | True: 0 | Pred: 1 ‚ùå\n",
      "     about           | True: 0 | Pred: 0 ‚úÖ\n",
      "     police          | True: 0 | Pred: 1 ‚ùå\n",
      "     interrogation   | True: 0 | Pred: 1 ‚ùå\n",
      "     ##s             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 1 ‚ùå\n",
      "     especially      | True: 0 | Pred: 1 ‚ùå\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     juveniles       | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     mentally        | True: 0 | Pred: 0 ‚úÖ\n",
      "     handicapped     | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 1 ‚ùå\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dataloader):\n\u001b[32m      7\u001b[39m     input_ids, attention_mask, target_index = [item.to(device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     probs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     preds = torch.argmax(torch.softmax(probs, dim=-\u001b[32m1\u001b[39m), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_ids.size(\u001b[32m0\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mFrameElementClassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# Encode sentence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         sentence_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m         token_embeddings = sentence_outputs.last_hidden_state  \u001b[38;5;66;03m# shape: (B, T, H)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m        # Encode role label (like \"Time\" or \"Manner\")\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:254\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLPProject/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_examples_to_print = 5  # or however many you want\n",
    "examples_printed = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(torch.softmax(probs, dim=-1), dim=-1)\n",
    "\n",
    "        for j in range(input_ids.size(0)):\n",
    "            if examples_printed >= num_examples_to_print:\n",
    "                break\n",
    "\n",
    "            input_id = input_ids[j]\n",
    "            attention = attention_mask[j]\n",
    "            pred = preds[j]\n",
    "            label = target_index[j]\n",
    "\n",
    "            # Only consider real (non-padding) tokens\n",
    "            mask = (attention == 1) & (label != -100)\n",
    "            input_id = input_id[mask]\n",
    "            pred = pred[mask]\n",
    "            label = label[mask]\n",
    "\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "            sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "            print(f\"\\nüîπ Sentence {examples_printed + 1}:\")\n",
    "            print(f\"   Text: {sentence}\")\n",
    "            print(f\"   Tokens: {tokens}\")\n",
    "            print(f\"   True Labels:     {label.tolist()}\")\n",
    "            print(f\"   Predicted Labels:{pred.tolist()}\")\n",
    "\n",
    "            # Optional: highlight mismatches\n",
    "            print(\"   Comparison:\")\n",
    "            for tok, gold, guess in zip(tokens, label.tolist(), pred.tolist()):\n",
    "                status = \"‚úÖ\" if gold == guess else \"‚ùå\"\n",
    "                print(f\"     {tok:15} | True: {gold} | Pred: {guess} {status}\")\n",
    "\n",
    "            examples_printed += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
