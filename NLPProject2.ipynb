{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPProject.ipynb  NLPProject2.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook metadata fixed! You can now commit to GitHub.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from google.colab import drive\n",
    "\n",
    "# Get the notebook's filename (usually matches the GitHub repo name)\n",
    "!ls *.ipynb\n",
    "notebook_name = \"NLPProject.ipynb\"  # ‚Üê Replace with your filename\n",
    "\n",
    "# Load and fix the notebook\n",
    "with open(notebook_name, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Option A: Remove widgets metadata completely (recommended)\n",
    "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Option B: Or add the missing state key\n",
    "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "#     nb['metadata']['widgets']['state'] = {}\n",
    "\n",
    "# Save the fixed version\n",
    "with open(notebook_name, 'w') as f:\n",
    "    json.dump(nb, f)\n",
    "\n",
    "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/kierstenwener/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Element: Time, Sample Sentences: 8170\n",
      "Frame Element: Manner, Sample Sentences: 7612\n",
      "Frame Element: Place, Sample Sentences: 7037\n",
      "Frame Element: Degree, Sample Sentences: 7012\n",
      "Frame Element: Means, Sample Sentences: 5045\n",
      "Frame Element: Explanation, Sample Sentences: 4539\n",
      "Frame Element: Depictive, Sample Sentences: 4091\n",
      "Frame Element: Purpose, Sample Sentences: 4091\n",
      "Frame Element: Circumstances, Sample Sentences: 3219\n",
      "Frame Element: Duration, Sample Sentences: 3120\n"
     ]
    }
   ],
   "source": [
    "frame_element_counts = {}\n",
    "#for each frame, loops through all frame elements\n",
    "for frame in fn.frames():\n",
    "    frame_name = frame.name\n",
    "\n",
    "    for fe_name, fe in frame.FE.items():\n",
    "\n",
    "        sample_sentences = frame.lexUnit\n",
    "        num_sentences = len(sample_sentences)\n",
    "\n",
    "        # Store the count of sentences for each frame element\n",
    "        if fe_name in frame_element_counts:\n",
    "            frame_element_counts[fe_name] += num_sentences  # Add the new count to the existing one\n",
    "        else:\n",
    "            frame_element_counts[fe_name] = num_sentences\n",
    "\n",
    "sorted_frame_elements = sorted(frame_element_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for fe_name, count in sorted_frame_elements[:10]:\n",
    "    print(f\"Frame Element: {fe_name}, Sample Sentences: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_with_time_ex = {}\n",
    "for f in fn.frames():\n",
    "    for x in f.FE:\n",
    "        if x == \"Time\":\n",
    "            frames_with_time_ex[f.name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, char_labels, offsets):\n",
    "    token_labels = []\n",
    "    for (token_start, token_end) in offsets:\n",
    "        # For special tokens like [CLS] and [SEP], offset is usually (0,0)\n",
    "        if token_start == token_end:\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # If any character in the token is marked as Time,\n",
    "            # decide on a label for the entire token.\n",
    "            token_tag = \"O\"\n",
    "            for pos in range(token_start, token_end):\n",
    "                if pos < len(char_labels) and char_labels[pos] != \"O\":\n",
    "                    token_tag = char_labels[pos]\n",
    "                    break\n",
    "            token_labels.append(token_tag)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "Input IDs: torch.Size([9013, 128])\n",
      "Attention Masks: torch.Size([9013, 128])\n",
      "Labels: torch.Size([9013, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.corpus import framenet as fn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Map BIO tags to IDs\n",
    "label2id = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Find frames that include \"Time\" as a frame element\n",
    "\n",
    "for name, frame in frames_with_time_ex.items():\n",
    "    # Print the frame name for reference\n",
    "    for lu in frame.lexUnit.values():\n",
    "        #print(f\"\\nLexical Unit: {lu['name']}\")\n",
    "        lu_data = fn.lu(lu['ID'])\n",
    "        for ex in lu_data['exemplars']:\n",
    "            text = ex['text']\n",
    "            char_labels = [\"O\"] * len(text)\n",
    "            has_time_fe = False\n",
    "\n",
    "            for fe in ex['FE']:\n",
    "                for i in fe:\n",
    "                    if i[2] == \"Time\":\n",
    "                        start, end = i[0], i[1]\n",
    "                        if start < end:\n",
    "                            char_labels[start] = \"B-Time\"\n",
    "                            for i in range(start+1, end):\n",
    "                                char_labels[i] = \"I-Time\"\n",
    "                            has_time_fe = True\n",
    "            if not has_time_fe:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Tokenize\n",
    "            tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # Map character-level labels to token-level labels\n",
    "            token_labels = align_labels_with_tokens(text, char_labels, offsets)\n",
    "            label2id_binary = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 1}  # Map both B-Time and I-Time to 1\n",
    "            # Pad remaining labels with -100 where attention mask is 0 (i.e., padding tokens)\n",
    "\n",
    "\n",
    "            label_ids = [label2id_binary.get(lab, 0) for lab in token_labels]\n",
    "            label_ids = [\n",
    "                label if mask == 1 else -100 \n",
    "                for label, mask in zip(label_ids, attention_mask)\n",
    "            ]\n",
    "            # Store tensors\n",
    "            input_ids_list.append(torch.tensor(input_ids))\n",
    "            attention_masks_list.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(label_ids))\n",
    "\n",
    "# Final dataset tensors\n",
    "input_ids_tensor = torch.stack(input_ids_list)\n",
    "attention_masks_tensor = torch.stack(attention_masks_list)\n",
    "labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "print(\"Tensor shapes:\")\n",
    "print(\"Input IDs:\", input_ids_tensor.shape)\n",
    "print(\"Attention Masks:\", attention_masks_tensor.shape)\n",
    "print(\"Labels:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: when\n"
     ]
    }
   ],
   "source": [
    "indices = (labels_tensor == 1).nonzero(as_tuple=False)\n",
    "sample_idx, token_idx = indices[0].tolist()\n",
    "token_id = input_ids_tensor[sample_idx][token_idx]\n",
    "token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "print(f\"Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'she', 'had', 'seen', 'no', 'reason', 'to', 'abandon', 'it', 'when', 'she', 'came', 'to', 'med', '##ew', '##ich', 'two', 'years', 'ago', ',', 'even', 'though', 'she', 'might', 'now', 'have', 'been', 'able', 'to', 'afford', 'a', 'car', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor[sample_idx])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           -> 0\n",
      "she             -> 0\n",
      "had             -> 0\n",
      "seen            -> 0\n",
      "no              -> 0\n",
      "reason          -> 0\n",
      "to              -> 0\n",
      "abandon         -> 0\n",
      "it              -> 0\n",
      "when            -> 1\n",
      "she             -> 1\n",
      "came            -> 1\n",
      "to              -> 1\n",
      "med             -> 1\n",
      "##ew            -> 1\n",
      "##ich           -> 1\n",
      "two             -> 1\n",
      "years           -> 1\n",
      "ago             -> 1\n",
      ",               -> 0\n",
      "even            -> 0\n",
      "though          -> 0\n",
      "she             -> 0\n",
      "might           -> 0\n",
      "now             -> 0\n",
      "have            -> 0\n",
      "been            -> 0\n",
      "able            -> 0\n",
      "to              -> 0\n",
      "afford          -> 0\n",
      "a               -> 0\n",
      "car             -> 0\n",
      ".               -> 0\n",
      "[SEP]           -> 0\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n"
     ]
    }
   ],
   "source": [
    "labels = labels_tensor[sample_idx]\n",
    "for tok, label in zip(tokens, labels):\n",
    "    print(f\"{tok:15} -> {label.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "validation_split = 0.5\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Shuffle the data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation (without shuffling)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SubsetRandomSampler(range(len(val_dataset))),  # Don't shuffle validation data\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class FrameElementClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        #self.query_encoder = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.token_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    #def forward(self, input_ids, attention_mask, role_ids, role_mask):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode sentence\n",
    "        sentence_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = sentence_outputs.last_hidden_state  # shape: (B, T, H)\n",
    "        \"\"\"\n",
    "        # Encode role label (like \"Time\" or \"Manner\")\n",
    "        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\n",
    "        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\n",
    "        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\n",
    "\"\"\"\n",
    "        # Project sentence tokens\n",
    "        token_embeddings = self.token_projection(token_embeddings)  # shape: (B, T, H)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "        # Compute dot product between role embedding and each token\n",
    "        #role_embedding = role_embedding.unsqueeze(2)  # (B, H, 1)\n",
    "        #scores = torch.bmm(token_embeddings, role_embedding).squeeze(-1)  # shape: (B, T)\n",
    "\n",
    "        # Optionally apply attention mask\n",
    "        #scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        #probs = torch.softmax(logits, dim=-1)\n",
    "        #logits = self.classifier(token_embeddings)\n",
    "        return logits  # Apply softmax for inference or use with CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m correct_predictions_batch = \u001b[32m0\u001b[39m\n\u001b[32m     23\u001b[39m total_predictions = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTarget label counts:\u001b[39m\u001b[33m\"\u001b[39m, torch.bincount(\u001b[43mtarget_index\u001b[49m.view(-\u001b[32m1\u001b[39m)))\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i >= num_batches:\n",
      "\u001b[31mNameError\u001b[39m: name 'target_index' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = FrameElementClassifier()\n",
    "\n",
    "# Set up device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "num_epochs = 17\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # Assuming target_index is 0 for \"Non-Time\" or 1 for \"Time\"\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Number of batches to train on\n",
    "num_batches = 5\n",
    "\n",
    "# To track accuracy\n",
    "accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    print(\"Target label counts:\", torch.bincount(target_index.view(-1)))\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        #print(\"Unique target values:\", torch.unique(target_index))\n",
    "        \n",
    "        # Zero the gradients before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        probs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_time_token = torch.argmax(time_probs, dim=1)  # (B,)\n",
    "        #print(f\"predicted time token: {predicted_time_token}\")\n",
    "\n",
    "        for b in range(target_index.size(0)):\n",
    "            time_positions = (target_index[b] == 1).nonzero(as_tuple=True)[0]\n",
    "            for token_idx in range(input_ids[b].size(0)):\n",
    "                token_id = input_ids[b][token_idx].item()\n",
    "                token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "                true_label = target_index[b][token_idx].item()\n",
    "                predicted_label = predicted_time_token[b].item()\n",
    "\n",
    "                #print(f\"Token: {token:15} -> True label: {true_label}, Predicted: {predicted_label}\")\n",
    "            for time_pos in time_positions:\n",
    "                true_token = time_pos.item()  # True token index\n",
    "                if predicted_time_token[b] == true_token:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        batch_accuracy = correct / total if total > 0 else 0\n",
    "        correct_predictions_batch += correct\n",
    "        total_predictions += total\n",
    "\n",
    "    epoch_accuracy = correct_predictions_batch / total_predictions\n",
    "    print(epoch_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 1, Validation Accuracy: 0.6668\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 2, Validation Accuracy: 0.6681\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 3, Validation Accuracy: 0.6600\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 4, Validation Accuracy: 0.6692\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 5, Validation Accuracy: 0.6709\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "accuracies = []\n",
    "num_batches = 100\n",
    "class_weights = torch.tensor([0.2, 0.8]).to(device)  # Make Time more important\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "model = FrameElementClassifier()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_positives = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "            probs = model(input_ids, attention_mask)\n",
    "            probs_softmax = torch.softmax(probs, dim=-1)\n",
    "            predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "            all_zeros_predicted = (predicted_tokens == 0).all()\n",
    "            mask = target_index != -100\n",
    "            #print(\"All predicted tokens are 0:\", all_zeros_predicted.item())      \n",
    "            #all_zeros_true = (target_index == 0).all()\n",
    "            #print(\"All true tokens are 0:\", all_zeros_true.item()) \n",
    "            correct += ((predicted_tokens == target_index) & mask).sum().item()\n",
    "            true_positives += ((predicted_tokens == 1) & (target_index == 1)).sum().item()\n",
    "            if true_positives > 0: print(\"Found a trie positive\")\n",
    "            total += mask.sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6668462159978454\n",
      "0.6680583122305113\n",
      "0.6600122640866662\n",
      "0.6692042896862174\n",
      "0.6708910891089109\n"
     ]
    }
   ],
   "source": [
    "for acc in accuracies:\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.1788 (361/2019)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Only count tokens that were labeled as \"Time\" (class 1) in the ground truth\n",
    "        is_time_token = (target_index == 1)\n",
    "        correct_time_preds = (predicted_tokens == 1) & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 2 ---\n",
      "Predicted : [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 3 ---\n",
      "Predicted : [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 1]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0]\n",
      "\n",
      "--- Example 4 ---\n",
      "Predicted : [0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      "True      : [0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 5 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      "True      : [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_printed = 0\n",
    "max_to_print = 5  # Adjust how many examples you want to print\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches or num_printed >= max_to_print:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Loop through each sentence in the batch\n",
    "        for b in range(input_ids.size(0)):\n",
    "            mask = target_index[b] != -100\n",
    "            true_labels = target_index[b][mask].cpu().numpy()\n",
    "            pred_labels = predicted_tokens[b][mask].cpu().numpy()\n",
    "\n",
    "            print(f\"\\n--- Example {num_printed + 1} ---\")\n",
    "            print(\"Predicted :\", pred_labels)\n",
    "            print(\"True      :\", true_labels)\n",
    "\n",
    "            num_printed += 1\n",
    "            if num_printed >= max_to_print:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Sentence 1:\n",
      "   Text: [CLS] before i settled among the tuthanach i wandered further up the river, crossing the great marsh. [SEP]\n",
      "   Tokens: ['[CLS]', 'before', 'i', 'settled', 'among', 'the', 'tu', '##than', '##ach', 'i', 'wandered', 'further', 'up', 'the', 'river', ',', 'crossing', 'the', 'great', 'marsh', '.', '[SEP]']\n",
      "   True Labels:     [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     before          | True: 1 | Pred: 0 ‚ùå\n",
      "     i               | True: 1 | Pred: 0 ‚ùå\n",
      "     settled         | True: 1 | Pred: 0 ‚ùå\n",
      "     among           | True: 1 | Pred: 0 ‚ùå\n",
      "     the             | True: 1 | Pred: 0 ‚ùå\n",
      "     tu              | True: 1 | Pred: 0 ‚ùå\n",
      "     ##than          | True: 1 | Pred: 0 ‚ùå\n",
      "     ##ach           | True: 1 | Pred: 0 ‚ùå\n",
      "     i               | True: 0 | Pred: 1 ‚ùå\n",
      "     wandered        | True: 0 | Pred: 0 ‚úÖ\n",
      "     further         | True: 0 | Pred: 0 ‚úÖ\n",
      "     up              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     river           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     crossing        | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     great           | True: 0 | Pred: 0 ‚úÖ\n",
      "     marsh           | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 2:\n",
      "   Text: [CLS] at breakfast next morning, the son of the house dashed past the open doorway stripped to the waist, shaving cream all over his face, looking very agitated. [SEP]\n",
      "   Tokens: ['[CLS]', 'at', 'breakfast', 'next', 'morning', ',', 'the', 'son', 'of', 'the', 'house', 'dashed', 'past', 'the', 'open', 'doorway', 'stripped', 'to', 'the', 'waist', ',', 'sha', '##ving', 'cream', 'all', 'over', 'his', 'face', ',', 'looking', 'very', 'agitated', '.', '[SEP]']\n",
      "   True Labels:     [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     at              | True: 1 | Pred: 0 ‚ùå\n",
      "     breakfast       | True: 1 | Pred: 1 ‚úÖ\n",
      "     next            | True: 1 | Pred: 1 ‚úÖ\n",
      "     morning         | True: 1 | Pred: 1 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 1 ‚ùå\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     son             | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     house           | True: 0 | Pred: 1 ‚ùå\n",
      "     dashed          | True: 0 | Pred: 1 ‚ùå\n",
      "     past            | True: 0 | Pred: 1 ‚ùå\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     open            | True: 0 | Pred: 1 ‚ùå\n",
      "     doorway         | True: 0 | Pred: 1 ‚ùå\n",
      "     stripped        | True: 0 | Pred: 0 ‚úÖ\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     waist           | True: 0 | Pred: 1 ‚ùå\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     sha             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ving          | True: 0 | Pred: 1 ‚ùå\n",
      "     cream           | True: 0 | Pred: 0 ‚úÖ\n",
      "     all             | True: 0 | Pred: 1 ‚ùå\n",
      "     over            | True: 0 | Pred: 0 ‚úÖ\n",
      "     his             | True: 0 | Pred: 0 ‚úÖ\n",
      "     face            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     looking         | True: 0 | Pred: 0 ‚úÖ\n",
      "     very            | True: 0 | Pred: 0 ‚úÖ\n",
      "     agitated        | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 1 ‚ùå\n",
      "\n",
      "üîπ Sentence 3:\n",
      "   Text: [CLS] since then, sales have jumped from 6. 86 billion francs to 8. 56 billion in 1992, while pre - tax profits have soared to 443 million francs from 130 million in 1988. [SEP]\n",
      "   Tokens: ['[CLS]', 'since', 'then', ',', 'sales', 'have', 'jumped', 'from', '6', '.', '86', 'billion', 'francs', 'to', '8', '.', '56', 'billion', 'in', '1992', ',', 'while', 'pre', '-', 'tax', 'profits', 'have', 'soared', 'to', '44', '##3', 'million', 'francs', 'from', '130', 'million', 'in', '1988', '.', '[SEP]']\n",
      "   True Labels:     [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     since           | True: 1 | Pred: 0 ‚ùå\n",
      "     then            | True: 1 | Pred: 0 ‚ùå\n",
      "     ,               | True: 0 | Pred: 1 ‚ùå\n",
      "     sales           | True: 0 | Pred: 0 ‚úÖ\n",
      "     have            | True: 0 | Pred: 0 ‚úÖ\n",
      "     jumped          | True: 0 | Pred: 1 ‚ùå\n",
      "     from            | True: 0 | Pred: 1 ‚ùå\n",
      "     6               | True: 0 | Pred: 1 ‚ùå\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     86              | True: 0 | Pred: 0 ‚úÖ\n",
      "     billion         | True: 0 | Pred: 1 ‚ùå\n",
      "     francs          | True: 0 | Pred: 0 ‚úÖ\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     8               | True: 0 | Pred: 1 ‚ùå\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     56              | True: 0 | Pred: 1 ‚ùå\n",
      "     billion         | True: 0 | Pred: 1 ‚ùå\n",
      "     in              | True: 1 | Pred: 1 ‚úÖ\n",
      "     1992            | True: 1 | Pred: 0 ‚ùå\n",
      "     ,               | True: 0 | Pred: 1 ‚ùå\n",
      "     while           | True: 0 | Pred: 0 ‚úÖ\n",
      "     pre             | True: 0 | Pred: 0 ‚úÖ\n",
      "     -               | True: 0 | Pred: 0 ‚úÖ\n",
      "     tax             | True: 0 | Pred: 0 ‚úÖ\n",
      "     profits         | True: 0 | Pred: 0 ‚úÖ\n",
      "     have            | True: 0 | Pred: 0 ‚úÖ\n",
      "     soared          | True: 0 | Pred: 1 ‚ùå\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     44              | True: 0 | Pred: 1 ‚ùå\n",
      "     ##3             | True: 0 | Pred: 1 ‚ùå\n",
      "     million         | True: 0 | Pred: 0 ‚úÖ\n",
      "     francs          | True: 0 | Pred: 0 ‚úÖ\n",
      "     from            | True: 0 | Pred: 0 ‚úÖ\n",
      "     130             | True: 0 | Pred: 1 ‚ùå\n",
      "     million         | True: 0 | Pred: 1 ‚ùå\n",
      "     in              | True: 0 | Pred: 1 ‚ùå\n",
      "     1988            | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 4:\n",
      "   Text: [CLS] milton roy disclosed in may that it was approached for a possible acquisition by thermo electron. [SEP]\n",
      "   Tokens: ['[CLS]', 'milton', 'roy', 'disclosed', 'in', 'may', 'that', 'it', 'was', 'approached', 'for', 'a', 'possible', 'acquisition', 'by', 'the', '##rm', '##o', 'electron', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     milton          | True: 0 | Pred: 1 ‚ùå\n",
      "     roy             | True: 0 | Pred: 1 ‚ùå\n",
      "     disclosed       | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 1 | Pred: 0 ‚ùå\n",
      "     may             | True: 1 | Pred: 1 ‚úÖ\n",
      "     that            | True: 0 | Pred: 0 ‚úÖ\n",
      "     it              | True: 0 | Pred: 1 ‚ùå\n",
      "     was             | True: 0 | Pred: 1 ‚ùå\n",
      "     approached      | True: 0 | Pred: 0 ‚úÖ\n",
      "     for             | True: 0 | Pred: 0 ‚úÖ\n",
      "     a               | True: 0 | Pred: 0 ‚úÖ\n",
      "     possible        | True: 0 | Pred: 0 ‚úÖ\n",
      "     acquisition     | True: 0 | Pred: 1 ‚ùå\n",
      "     by              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##rm            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##o             | True: 0 | Pred: 0 ‚úÖ\n",
      "     electron        | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 1 ‚ùå\n",
      "\n",
      "üîπ Sentence 5:\n",
      "   Text: [CLS] in may the price of cocoa fell to its lowest level since 1975 - 76. [SEP]\n",
      "   Tokens: ['[CLS]', 'in', 'may', 'the', 'price', 'of', 'cocoa', 'fell', 'to', 'its', 'lowest', 'level', 'since', '1975', '-', '76', '.', '[SEP]']\n",
      "   True Labels:     [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 1 | Pred: 0 ‚ùå\n",
      "     may             | True: 1 | Pred: 0 ‚ùå\n",
      "     the             | True: 0 | Pred: 1 ‚ùå\n",
      "     price           | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     cocoa           | True: 0 | Pred: 1 ‚ùå\n",
      "     fell            | True: 0 | Pred: 1 ‚ùå\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     its             | True: 0 | Pred: 1 ‚ùå\n",
      "     lowest          | True: 0 | Pred: 1 ‚ùå\n",
      "     level           | True: 0 | Pred: 1 ‚ùå\n",
      "     since           | True: 0 | Pred: 1 ‚ùå\n",
      "     1975            | True: 0 | Pred: 0 ‚úÖ\n",
      "     -               | True: 0 | Pred: 0 ‚úÖ\n",
      "     76              | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 1 ‚ùå\n",
      "     [SEP]           | True: 0 | Pred: 1 ‚ùå\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_examples_to_print = 5  # or however many you want\n",
    "examples_printed = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(torch.softmax(probs, dim=-1), dim=-1)\n",
    "\n",
    "        for j in range(input_ids.size(0)):\n",
    "            if examples_printed >= num_examples_to_print:\n",
    "                break\n",
    "\n",
    "            input_id = input_ids[j]\n",
    "            attention = attention_mask[j]\n",
    "            pred = preds[j]\n",
    "            label = target_index[j]\n",
    "\n",
    "            # Only consider real (non-padding) tokens\n",
    "            mask = (attention == 1) & (label != -100)\n",
    "            input_id = input_id[mask]\n",
    "            pred = pred[mask]\n",
    "            label = label[mask]\n",
    "\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "            sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "            print(f\"\\nüîπ Sentence {examples_printed + 1}:\")\n",
    "            print(f\"   Text: {sentence}\")\n",
    "            print(f\"   Tokens: {tokens}\")\n",
    "            print(f\"   True Labels:     {label.tolist()}\")\n",
    "            print(f\"   Predicted Labels:{pred.tolist()}\")\n",
    "\n",
    "            # Optional: highlight mismatches\n",
    "            print(\"   Comparison:\")\n",
    "            for tok, gold, guess in zip(tokens, label.tolist(), pred.tolist()):\n",
    "                status = \"‚úÖ\" if gold == guess else \"‚ùå\"\n",
    "                print(f\"     {tok:15} | True: {gold} | Pred: {guess} {status}\")\n",
    "\n",
    "            examples_printed += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
