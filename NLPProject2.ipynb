{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPProject.ipynb  NLPProject2.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook metadata fixed! You can now commit to GitHub.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from google.colab import drive\n",
    "\n",
    "# Get the notebook's filename (usually matches the GitHub repo name)\n",
    "!ls *.ipynb\n",
    "notebook_name = \"NLPProject.ipynb\"  # ‚Üê Replace with your filename\n",
    "\n",
    "# Load and fix the notebook\n",
    "with open(notebook_name, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Option A: Remove widgets metadata completely (recommended)\n",
    "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Option B: Or add the missing state key\n",
    "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "#     nb['metadata']['widgets']['state'] = {}\n",
    "\n",
    "# Save the fixed version\n",
    "with open(notebook_name, 'w') as f:\n",
    "    json.dump(nb, f)\n",
    "\n",
    "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/kierstenwener/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Element: Time, Sample Sentences: 8170\n",
      "Frame Element: Manner, Sample Sentences: 7612\n",
      "Frame Element: Place, Sample Sentences: 7037\n",
      "Frame Element: Degree, Sample Sentences: 7012\n",
      "Frame Element: Means, Sample Sentences: 5045\n",
      "Frame Element: Explanation, Sample Sentences: 4539\n",
      "Frame Element: Depictive, Sample Sentences: 4091\n",
      "Frame Element: Purpose, Sample Sentences: 4091\n",
      "Frame Element: Circumstances, Sample Sentences: 3219\n",
      "Frame Element: Duration, Sample Sentences: 3120\n"
     ]
    }
   ],
   "source": [
    "frame_element_counts = {}\n",
    "#for each frame, loops through all frame elements\n",
    "for frame in fn.frames():\n",
    "    frame_name = frame.name\n",
    "\n",
    "    for fe_name, fe in frame.FE.items():\n",
    "\n",
    "        sample_sentences = frame.lexUnit\n",
    "        num_sentences = len(sample_sentences)\n",
    "\n",
    "        # Store the count of sentences for each frame element\n",
    "        if fe_name in frame_element_counts:\n",
    "            frame_element_counts[fe_name] += num_sentences  # Add the new count to the existing one\n",
    "        else:\n",
    "            frame_element_counts[fe_name] = num_sentences\n",
    "\n",
    "sorted_frame_elements = sorted(frame_element_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for fe_name, count in sorted_frame_elements[:10]:\n",
    "    print(f\"Frame Element: {fe_name}, Sample Sentences: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_with_time_ex = {}\n",
    "for f in fn.frames():\n",
    "    for x in f.FE:\n",
    "        if x == \"Time\":\n",
    "            frames_with_time_ex[f.name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, char_labels, offsets):\n",
    "    token_labels = []\n",
    "    for (token_start, token_end) in offsets:\n",
    "        # For special tokens like [CLS] and [SEP], offset is usually (0,0)\n",
    "        if token_start == token_end:\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # If any character in the token is marked as Time,\n",
    "            # decide on a label for the entire token.\n",
    "            token_tag = \"O\"\n",
    "            for pos in range(token_start, token_end):\n",
    "                if pos < len(char_labels) and char_labels[pos] != \"O\":\n",
    "                    token_tag = char_labels[pos]\n",
    "                    break\n",
    "            token_labels.append(token_tag)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "Input IDs: torch.Size([123983, 128])\n",
      "Attention Masks: torch.Size([123983, 128])\n",
      "Labels: torch.Size([123983, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.corpus import framenet as fn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Map BIO tags to IDs\n",
    "label2id = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Find frames that include \"Time\" as a frame element\n",
    "\n",
    "for name, frame in frames_with_time_ex.items():\n",
    "    # Print the frame name for reference\n",
    "    for lu in frame.lexUnit.values():\n",
    "        #print(f\"\\nLexical Unit: {lu['name']}\")\n",
    "        lu_data = fn.lu(lu['ID'])\n",
    "        for ex in lu_data['exemplars']:\n",
    "            text = ex['text']\n",
    "            char_labels = [\"O\"] * len(text)\n",
    "\n",
    "            for fe in ex['FE']:\n",
    "                for i in fe:\n",
    "                    if i[2] == \"Time\":\n",
    "                        start, end = i[0], i[1]\n",
    "                        if start < end:\n",
    "                            char_labels[start] = \"B-Time\"\n",
    "                            for i in range(start+1, end):\n",
    "                                char_labels[i] = \"I-Time\"\n",
    "\n",
    "\n",
    "            # Tokenize\n",
    "            tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # Map character-level labels to token-level labels\n",
    "            token_labels = align_labels_with_tokens(text, char_labels, offsets)\n",
    "            label2id_binary = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 1}  # Map both B-Time and I-Time to 1\n",
    "            # Pad remaining labels with -100 where attention mask is 0 (i.e., padding tokens)\n",
    "\n",
    "\n",
    "            label_ids = [label2id_binary.get(lab, 0) for lab in token_labels]\n",
    "            label_ids = [\n",
    "                label if mask == 1 else -100 \n",
    "                for label, mask in zip(label_ids, attention_mask)\n",
    "            ]\n",
    "            # Store tensors\n",
    "            input_ids_list.append(torch.tensor(input_ids))\n",
    "            attention_masks_list.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(label_ids))\n",
    "\n",
    "# Final dataset tensors\n",
    "input_ids_tensor = torch.stack(input_ids_list)\n",
    "attention_masks_tensor = torch.stack(attention_masks_list)\n",
    "labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "print(\"Tensor shapes:\")\n",
    "print(\"Input IDs:\", input_ids_tensor.shape)\n",
    "print(\"Attention Masks:\", attention_masks_tensor.shape)\n",
    "print(\"Labels:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: when\n"
     ]
    }
   ],
   "source": [
    "indices = (labels_tensor == 1).nonzero(as_tuple=False)\n",
    "sample_idx, token_idx = indices[0].tolist()\n",
    "token_id = input_ids_tensor[sample_idx][token_idx]\n",
    "token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "print(f\"Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'she', 'had', 'seen', 'no', 'reason', 'to', 'abandon', 'it', 'when', 'she', 'came', 'to', 'med', '##ew', '##ich', 'two', 'years', 'ago', ',', 'even', 'though', 'she', 'might', 'now', 'have', 'been', 'able', 'to', 'afford', 'a', 'car', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor[sample_idx])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           -> 0\n",
      "she             -> 0\n",
      "had             -> 0\n",
      "seen            -> 0\n",
      "no              -> 0\n",
      "reason          -> 0\n",
      "to              -> 0\n",
      "abandon         -> 0\n",
      "it              -> 0\n",
      "when            -> 1\n",
      "she             -> 1\n",
      "came            -> 1\n",
      "to              -> 1\n",
      "med             -> 1\n",
      "##ew            -> 1\n",
      "##ich           -> 1\n",
      "two             -> 1\n",
      "years           -> 1\n",
      "ago             -> 1\n",
      ",               -> 0\n",
      "even            -> 0\n",
      "though          -> 0\n",
      "she             -> 0\n",
      "might           -> 0\n",
      "now             -> 0\n",
      "have            -> 0\n",
      "been            -> 0\n",
      "able            -> 0\n",
      "to              -> 0\n",
      "afford          -> 0\n",
      "a               -> 0\n",
      "car             -> 0\n",
      ".               -> 0\n",
      "[SEP]           -> 0\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n"
     ]
    }
   ],
   "source": [
    "labels = labels_tensor[sample_idx]\n",
    "for tok, label in zip(tokens, labels):\n",
    "    print(f\"{tok:15} -> {label.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "validation_split = 0.5\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Shuffle the data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation (without shuffling)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SubsetRandomSampler(range(len(val_dataset))),  # Don't shuffle validation data\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class FrameElementClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        #self.query_encoder = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.token_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    #def forward(self, input_ids, attention_mask, role_ids, role_mask):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode sentence\n",
    "        sentence_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = sentence_outputs.last_hidden_state  # shape: (B, T, H)\n",
    "        \"\"\"\n",
    "        # Encode role label (like \"Time\" or \"Manner\")\n",
    "        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\n",
    "        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\n",
    "        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\n",
    "\"\"\"\n",
    "        # Project sentence tokens\n",
    "        token_embeddings = self.token_projection(token_embeddings)  # shape: (B, T, H)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "        # Compute dot product between role embedding and each token\n",
    "        #role_embedding = role_embedding.unsqueeze(2)  # (B, H, 1)\n",
    "        #scores = torch.bmm(token_embeddings, role_embedding).squeeze(-1)  # shape: (B, T)\n",
    "\n",
    "        # Optionally apply attention mask\n",
    "        #scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        #probs = torch.softmax(logits, dim=-1)\n",
    "        #logits = self.classifier(token_embeddings)\n",
    "        return logits  # Apply softmax for inference or use with CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m correct_predictions_batch = \u001b[32m0\u001b[39m\n\u001b[32m     23\u001b[39m total_predictions = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTarget label counts:\u001b[39m\u001b[33m\"\u001b[39m, torch.bincount(\u001b[43mtarget_index\u001b[49m.view(-\u001b[32m1\u001b[39m)))\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i >= num_batches:\n",
      "\u001b[31mNameError\u001b[39m: name 'target_index' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = FrameElementClassifier()\n",
    "\n",
    "# Set up device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "num_epochs = 17\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # Assuming target_index is 0 for \"Non-Time\" or 1 for \"Time\"\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Number of batches to train on\n",
    "num_batches = 5\n",
    "\n",
    "# To track accuracy\n",
    "accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    print(\"Target label counts:\", torch.bincount(target_index.view(-1)))\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        #print(\"Unique target values:\", torch.unique(target_index))\n",
    "        \n",
    "        # Zero the gradients before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        probs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_time_token = torch.argmax(time_probs, dim=1)  # (B,)\n",
    "        #print(f\"predicted time token: {predicted_time_token}\")\n",
    "\n",
    "        for b in range(target_index.size(0)):\n",
    "            time_positions = (target_index[b] == 1).nonzero(as_tuple=True)[0]\n",
    "            for token_idx in range(input_ids[b].size(0)):\n",
    "                token_id = input_ids[b][token_idx].item()\n",
    "                token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "                true_label = target_index[b][token_idx].item()\n",
    "                predicted_label = predicted_time_token[b].item()\n",
    "\n",
    "                #print(f\"Token: {token:15} -> True label: {true_label}, Predicted: {predicted_label}\")\n",
    "            for time_pos in time_positions:\n",
    "                true_token = time_pos.item()  # True token index\n",
    "                if predicted_time_token[b] == true_token:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        batch_accuracy = correct / total if total > 0 else 0\n",
    "        correct_predictions_batch += correct\n",
    "        total_predictions += total\n",
    "\n",
    "    epoch_accuracy = correct_predictions_batch / total_predictions\n",
    "    print(epoch_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 1, Validation Accuracy: 0.3786\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 2, Validation Accuracy: 0.3804\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 3, Validation Accuracy: 0.3659\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 4, Validation Accuracy: 0.3736\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Found a trie positive\n",
      "Epoch 5, Validation Accuracy: 0.3699\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "accuracies = []\n",
    "num_batches = 100\n",
    "class_weights = torch.tensor([0.2, 0.8]).to(device)  # Make Time more important\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "model = FrameElementClassifier()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_positives = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "            probs = model(input_ids, attention_mask)\n",
    "            probs_softmax = torch.softmax(probs, dim=-1)\n",
    "            predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "            all_zeros_predicted = (predicted_tokens == 0).all()\n",
    "            mask = target_index != -100\n",
    "            #print(\"All predicted tokens are 0:\", all_zeros_predicted.item())      \n",
    "            #all_zeros_true = (target_index == 0).all()\n",
    "            #print(\"All true tokens are 0:\", all_zeros_true.item()) \n",
    "            correct += ((predicted_tokens == target_index) & mask).sum().item()\n",
    "            true_positives += ((predicted_tokens == 1) & (target_index == 1)).sum().item()\n",
    "            if true_positives > 0: print(\"Found a trie positive\")\n",
    "            total += mask.sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37857447716602644\n",
      "0.38038345972242094\n",
      "0.3658570533788016\n",
      "0.37362714728245566\n",
      "0.3699207676092171\n"
     ]
    }
   ],
   "source": [
    "for acc in accuracies:\n",
    "    print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
