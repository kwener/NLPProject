{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook metadata fixed! You can now commit to GitHub.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from google.colab import drive\n",
    "\n",
    "# Get the notebook's filename (usually matches the GitHub repo name)\n",
    "!ls *.ipynb\n",
    "notebook_name = \"NLPProject.ipynb\"  # ‚Üê Replace with your filename\n",
    "\n",
    "# Load and fix the notebook\n",
    "with open(notebook_name, 'r') as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Option A: Remove widgets metadata completely (recommended)\n",
    "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Option B: Or add the missing state key\n",
    "# if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
    "#     nb['metadata']['widgets']['state'] = {}\n",
    "\n",
    "# Save the fixed version\n",
    "with open(notebook_name, 'w') as f:\n",
    "    json.dump(nb, f)\n",
    "\n",
    "print(\"Notebook metadata fixed! You can now commit to GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     C:\\Users\\ogboi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Element: Time, Sample Sentences: 8170\n",
      "Frame Element: Manner, Sample Sentences: 7612\n",
      "Frame Element: Place, Sample Sentences: 7037\n",
      "Frame Element: Degree, Sample Sentences: 7012\n",
      "Frame Element: Means, Sample Sentences: 5045\n",
      "Frame Element: Explanation, Sample Sentences: 4539\n",
      "Frame Element: Depictive, Sample Sentences: 4091\n",
      "Frame Element: Purpose, Sample Sentences: 4091\n",
      "Frame Element: Circumstances, Sample Sentences: 3219\n",
      "Frame Element: Duration, Sample Sentences: 3120\n"
     ]
    }
   ],
   "source": [
    "frame_element_counts = {}\n",
    "#for each frame, loops through all frame elements\n",
    "for frame in fn.frames():\n",
    "    frame_name = frame.name\n",
    "\n",
    "    for fe_name, fe in frame.FE.items():\n",
    "\n",
    "        sample_sentences = frame.lexUnit\n",
    "        num_sentences = len(sample_sentences)\n",
    "\n",
    "        # Store the count of sentences for each frame element\n",
    "        if fe_name in frame_element_counts:\n",
    "            frame_element_counts[fe_name] += num_sentences  # Add the new count to the existing one\n",
    "        else:\n",
    "            frame_element_counts[fe_name] = num_sentences\n",
    "\n",
    "sorted_frame_elements = sorted(frame_element_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for fe_name, count in sorted_frame_elements[:10]:\n",
    "    print(f\"Frame Element: {fe_name}, Sample Sentences: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_with_time_ex = {}\n",
    "for f in fn.frames():\n",
    "    for x in f.FE:\n",
    "        if x == \"Time\":\n",
    "            frames_with_time_ex[f.name] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, char_labels, offsets):\n",
    "    token_labels = []\n",
    "    for (token_start, token_end) in offsets:\n",
    "        # For special tokens like [CLS] and [SEP], offset is usually (0,0)\n",
    "        if token_start == token_end:\n",
    "            token_labels.append(\"O\")\n",
    "        else:\n",
    "            # If any character in the token is marked as Time,\n",
    "            # decide on a label for the entire token.\n",
    "            token_tag = \"O\"\n",
    "            for pos in range(token_start, token_end):\n",
    "                if pos < len(char_labels) and char_labels[pos] != \"O\":\n",
    "                    token_tag = char_labels[pos]\n",
    "                    break\n",
    "            token_labels.append(token_tag)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.corpus import framenet as fn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Map BIO tags to IDs\n",
    "label2id = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 2}\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Find frames that include \"Time\" as a frame element\n",
    "\n",
    "for name, frame in frames_with_time_ex.items():\n",
    "    # Print the frame name for reference\n",
    "    for lu in frame.lexUnit.values():\n",
    "        #print(f\"\\nLexical Unit: {lu['name']}\")\n",
    "        lu_data = fn.lu(lu['ID'])\n",
    "        for ex in lu_data['exemplars']:\n",
    "            text = ex['text']\n",
    "            char_labels = [\"O\"] * len(text)\n",
    "            has_time_fe = False\n",
    "\n",
    "            for fe in ex['FE']:\n",
    "                for i in fe:\n",
    "                    if i[2] == \"Time\":\n",
    "                        start, end = i[0], i[1]\n",
    "                        if start < end:\n",
    "                            char_labels[start] = \"B-Time\"\n",
    "                            for i in range(start+1, end):\n",
    "                                char_labels[i] = \"I-Time\"\n",
    "                            has_time_fe = True\n",
    "            if not has_time_fe:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Tokenize\n",
    "            tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "            input_ids = tokenized[\"input_ids\"]\n",
    "            attention_mask = tokenized[\"attention_mask\"]\n",
    "            offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "            # Map character-level labels to token-level labels\n",
    "            token_labels = align_labels_with_tokens(text, char_labels, offsets)\n",
    "            label2id_binary = {\"O\": 0, \"B-Time\": 1, \"I-Time\": 1}  # Map both B-Time and I-Time to 1\n",
    "            # Pad remaining labels with -100 where attention mask is 0 (i.e., padding tokens)\n",
    "\n",
    "\n",
    "            label_ids = [label2id_binary.get(lab, 0) for lab in token_labels]\n",
    "            label_ids = [\n",
    "                label if mask == 1 else -100 \n",
    "                for label, mask in zip(label_ids, attention_mask)\n",
    "            ]\n",
    "            # Store tensors\n",
    "            input_ids_list.append(torch.tensor(input_ids))\n",
    "            attention_masks_list.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(label_ids))\n",
    "\n",
    "# Final dataset tensors\n",
    "input_ids_tensor = torch.stack(input_ids_list)\n",
    "attention_masks_tensor = torch.stack(attention_masks_list)\n",
    "labels_tensor = torch.stack(labels_list)\n",
    "\n",
    "print(\"Tensor shapes:\")\n",
    "print(\"Input IDs:\", input_ids_tensor.shape)\n",
    "print(\"Attention Masks:\", attention_masks_tensor.shape)\n",
    "print(\"Labels:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: when\n"
     ]
    }
   ],
   "source": [
    "indices = (labels_tensor == 1).nonzero(as_tuple=False)\n",
    "sample_idx, token_idx = indices[0].tolist()\n",
    "token_id = input_ids_tensor[sample_idx][token_idx]\n",
    "token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "print(f\"Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'she', 'had', 'seen', 'no', 'reason', 'to', 'abandon', 'it', 'when', 'she', 'came', 'to', 'med', '##ew', '##ich', 'two', 'years', 'ago', ',', 'even', 'though', 'she', 'might', 'now', 'have', 'been', 'able', 'to', 'afford', 'a', 'car', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_tensor[sample_idx])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           -> 0\n",
      "she             -> 0\n",
      "had             -> 0\n",
      "seen            -> 0\n",
      "no              -> 0\n",
      "reason          -> 0\n",
      "to              -> 0\n",
      "abandon         -> 0\n",
      "it              -> 0\n",
      "when            -> 1\n",
      "she             -> 1\n",
      "came            -> 1\n",
      "to              -> 1\n",
      "med             -> 1\n",
      "##ew            -> 1\n",
      "##ich           -> 1\n",
      "two             -> 1\n",
      "years           -> 1\n",
      "ago             -> 1\n",
      ",               -> 0\n",
      "even            -> 0\n",
      "though          -> 0\n",
      "she             -> 0\n",
      "might           -> 0\n",
      "now             -> 0\n",
      "have            -> 0\n",
      "been            -> 0\n",
      "able            -> 0\n",
      "to              -> 0\n",
      "afford          -> 0\n",
      "a               -> 0\n",
      "car             -> 0\n",
      ".               -> 0\n",
      "[SEP]           -> 0\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n",
      "[PAD]           -> -100\n"
     ]
    }
   ],
   "source": [
    "labels = labels_tensor[sample_idx]\n",
    "for tok, label in zip(tokens, labels):\n",
    "    print(f\"{tok:15} -> {label.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "validation_split = 0.5\n",
    "\n",
    "train_size = int((1 - validation_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Shuffle the data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation (without shuffling)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SubsetRandomSampler(range(len(val_dataset))),  # Don't shuffle validation data\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class FrameElementClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        #self.query_encoder = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.token_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    #def forward(self, input_ids, attention_mask, role_ids, role_mask):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode sentence\n",
    "        sentence_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = sentence_outputs.last_hidden_state  # shape: (B, T, H)\n",
    "        \"\"\"\n",
    "        # Encode role label (like \"Time\" or \"Manner\")\n",
    "        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\n",
    "        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\n",
    "        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\n",
    "\"\"\"\n",
    "        # Project sentence tokens\n",
    "        token_embeddings = self.token_projection(token_embeddings)  # shape: (B, T, H)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "        # Compute dot product between role embedding and each token\n",
    "        #role_embedding = role_embedding.unsqueeze(2)  # (B, H, 1)\n",
    "        #scores = torch.bmm(token_embeddings, role_embedding).squeeze(-1)  # shape: (B, T)\n",
    "\n",
    "        # Optionally apply attention mask\n",
    "        #scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        #probs = torch.softmax(logits, dim=-1)\n",
    "        #logits = self.classifier(token_embeddings)\n",
    "        return logits  # Apply softmax for inference or use with CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for a few more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "accuracies = []\n",
    "num_batches = 15\n",
    "model = FrameElementClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor([0.4, 0.6]).to(device)  # Make Time more important\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions_batch = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(i)\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(probs.view(-1, 2), target_index.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.7484 (235/314)\n",
      "Confusion Matrix (for 'Time' class prediction):\n",
      "[[8871  415]\n",
      " [  79  235]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "# For confusion matrix\n",
    "all_true_binary = []\n",
    "all_pred_binary = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Binary labels: 1 for \"Time\", 0 for everything else\n",
    "        is_time_token = (target_index == 1)\n",
    "        predicted_time_token = (predicted_tokens == 1)\n",
    "\n",
    "        correct_time_preds = predicted_time_token & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "        # Flatten and convert to binary 0/1\n",
    "        all_true_binary.extend(is_time_token.view(-1).cpu().numpy())\n",
    "        all_pred_binary.extend(predicted_time_token.view(-1).cpu().numpy())\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_true_binary, all_pred_binary)\n",
    "print(\"Confusion Matrix (for 'Time' class prediction):\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Time FE Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix: \n",
    "\n",
    "\n",
    "\t              | Predicted: Not Time (0) |\tPredicted: Time (1)\n",
    "\n",
    "\n",
    "                  Actual: Not Time (0) | 8442\t              | 889\n",
    "\n",
    "\n",
    "                   Actual: Time (1)  |\t14\t                  |255\n",
    "\n",
    "Model on average is good at finding time frame elements, though its not perfectly precise ( a lot of not time elements are still be declared as time elements )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time FE Recall: 0.7092 (217/306)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true_time_total = 0\n",
    "true_time_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Only count tokens that were labeled as \"Time\" (class 1) in the ground truth\n",
    "        is_time_token = (target_index == 1)\n",
    "        correct_time_preds = (predicted_tokens == 1) & is_time_token\n",
    "\n",
    "        true_time_total += is_time_token.sum().item()\n",
    "        true_time_correct += correct_time_preds.sum().item()\n",
    "\n",
    "if true_time_total > 0:\n",
    "    time_recall = true_time_correct / true_time_total\n",
    "    print(f\"Time FE Recall: {time_recall:.4f} ({true_time_correct}/{true_time_total})\")\n",
    "else:\n",
    "    print(\"No Time FEs found in validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Predicted : [0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 2 ---\n",
      "Predicted : [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 3 ---\n",
      "Predicted : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 1 0 1 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0]\n",
      "\n",
      "--- Example 4 ---\n",
      "Predicted : [0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "--- Example 5 ---\n",
      "Predicted : [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True      : [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_printed = 0\n",
    "max_to_print = 5  # Adjust how many examples you want to print\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_batches or num_printed >= max_to_print:\n",
    "            break\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        probs_softmax = torch.softmax(probs, dim=-1)\n",
    "        predicted_tokens = torch.argmax(probs_softmax, dim=-1)\n",
    "\n",
    "        # Loop through each sentence in the batch\n",
    "        for b in range(input_ids.size(0)):\n",
    "            mask = target_index[b] != -100\n",
    "            true_labels = target_index[b][mask].cpu().numpy()\n",
    "            pred_labels = predicted_tokens[b][mask].cpu().numpy()\n",
    "\n",
    "            print(f\"\\n--- Example {num_printed + 1} ---\")\n",
    "            print(\"Predicted :\", pred_labels)\n",
    "            print(\"True      :\", true_labels)\n",
    "\n",
    "            num_printed += 1\n",
    "            if num_printed >= max_to_print:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Sentence 1:\n",
      "   Text: [CLS] in this trial, a jury of seven men and five women heard five weeks of testimony and deliberated nine hours before reaching a unanimous verdict of guilty. [SEP]\n",
      "   Tokens: ['[CLS]', 'in', 'this', 'trial', ',', 'a', 'jury', 'of', 'seven', 'men', 'and', 'five', 'women', 'heard', 'five', 'weeks', 'of', 'testimony', 'and', 'deliberate', '##d', 'nine', 'hours', 'before', 'reaching', 'a', 'unanimous', 'verdict', 'of', 'guilty', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 0 | Pred: 0 ‚úÖ\n",
      "     this            | True: 0 | Pred: 0 ‚úÖ\n",
      "     trial           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     a               | True: 0 | Pred: 0 ‚úÖ\n",
      "     jury            | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     seven           | True: 0 | Pred: 0 ‚úÖ\n",
      "     men             | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     five            | True: 0 | Pred: 0 ‚úÖ\n",
      "     women           | True: 0 | Pred: 0 ‚úÖ\n",
      "     heard           | True: 0 | Pred: 0 ‚úÖ\n",
      "     five            | True: 0 | Pred: 0 ‚úÖ\n",
      "     weeks           | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     testimony       | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     deliberate      | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##d             | True: 0 | Pred: 0 ‚úÖ\n",
      "     nine            | True: 0 | Pred: 0 ‚úÖ\n",
      "     hours           | True: 0 | Pred: 1 ‚ùå\n",
      "     before          | True: 1 | Pred: 1 ‚úÖ\n",
      "     reaching        | True: 1 | Pred: 1 ‚úÖ\n",
      "     a               | True: 1 | Pred: 1 ‚úÖ\n",
      "     unanimous       | True: 1 | Pred: 1 ‚úÖ\n",
      "     verdict         | True: 1 | Pred: 1 ‚úÖ\n",
      "     of              | True: 1 | Pred: 1 ‚úÖ\n",
      "     guilty          | True: 1 | Pred: 1 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 2:\n",
      "   Text: [CLS] since sri lanka previously exported about 200, 000 tonnes of tea annually, only a little less than india, the biggest exporter, the loss is crucial. [SEP]\n",
      "   Tokens: ['[CLS]', 'since', 'sri', 'lanka', 'previously', 'exported', 'about', '200', ',', '000', 'tonnes', 'of', 'tea', 'annually', ',', 'only', 'a', 'little', 'less', 'than', 'india', ',', 'the', 'biggest', 'export', '##er', ',', 'the', 'loss', 'is', 'crucial', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     since           | True: 0 | Pred: 0 ‚úÖ\n",
      "     sri             | True: 0 | Pred: 0 ‚úÖ\n",
      "     lanka           | True: 0 | Pred: 0 ‚úÖ\n",
      "     previously      | True: 1 | Pred: 1 ‚úÖ\n",
      "     exported        | True: 0 | Pred: 0 ‚úÖ\n",
      "     about           | True: 0 | Pred: 0 ‚úÖ\n",
      "     200             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     000             | True: 0 | Pred: 0 ‚úÖ\n",
      "     tonnes          | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     tea             | True: 0 | Pred: 0 ‚úÖ\n",
      "     annually        | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     only            | True: 0 | Pred: 0 ‚úÖ\n",
      "     a               | True: 0 | Pred: 0 ‚úÖ\n",
      "     little          | True: 0 | Pred: 0 ‚úÖ\n",
      "     less            | True: 0 | Pred: 0 ‚úÖ\n",
      "     than            | True: 0 | Pred: 0 ‚úÖ\n",
      "     india           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     biggest         | True: 0 | Pred: 0 ‚úÖ\n",
      "     export          | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##er            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     loss            | True: 0 | Pred: 0 ‚úÖ\n",
      "     is              | True: 0 | Pred: 0 ‚úÖ\n",
      "     crucial         | True: 0 | Pred: 0 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 3:\n",
      "   Text: [CLS] 1179 singleton perinatal deaths and their selected live born controls among 114362 singleton births to women whose place of residence was leicestershire during 1978 - 87. [SEP]\n",
      "   Tokens: ['[CLS]', '117', '##9', 'singleton', 'per', '##ina', '##tal', 'deaths', 'and', 'their', 'selected', 'live', 'born', 'controls', 'among', '114', '##36', '##2', 'singleton', 'births', 'to', 'women', 'whose', 'place', 'of', 'residence', 'was', 'leicestershire', 'during', '1978', '-', '87', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     117             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##9             | True: 0 | Pred: 0 ‚úÖ\n",
      "     singleton       | True: 0 | Pred: 0 ‚úÖ\n",
      "     per             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##ina           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##tal           | True: 0 | Pred: 0 ‚úÖ\n",
      "     deaths          | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     their           | True: 0 | Pred: 0 ‚úÖ\n",
      "     selected        | True: 0 | Pred: 0 ‚úÖ\n",
      "     live            | True: 0 | Pred: 0 ‚úÖ\n",
      "     born            | True: 0 | Pred: 0 ‚úÖ\n",
      "     controls        | True: 0 | Pred: 0 ‚úÖ\n",
      "     among           | True: 0 | Pred: 0 ‚úÖ\n",
      "     114             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##36            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##2             | True: 0 | Pred: 0 ‚úÖ\n",
      "     singleton       | True: 0 | Pred: 0 ‚úÖ\n",
      "     births          | True: 0 | Pred: 0 ‚úÖ\n",
      "     to              | True: 0 | Pred: 0 ‚úÖ\n",
      "     women           | True: 0 | Pred: 0 ‚úÖ\n",
      "     whose           | True: 0 | Pred: 0 ‚úÖ\n",
      "     place           | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     residence       | True: 0 | Pred: 0 ‚úÖ\n",
      "     was             | True: 0 | Pred: 0 ‚úÖ\n",
      "     leicestershire  | True: 0 | Pred: 0 ‚úÖ\n",
      "     during          | True: 1 | Pred: 1 ‚úÖ\n",
      "     1978            | True: 1 | Pred: 1 ‚úÖ\n",
      "     -               | True: 1 | Pred: 1 ‚úÖ\n",
      "     87              | True: 1 | Pred: 1 ‚úÖ\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 4:\n",
      "   Text: [CLS] morsi and other brotherhood leaders were arrested jan. 28 and held in the wadi natroun prison north of cairo, until they escaped two days later. [SEP]\n",
      "   Tokens: ['[CLS]', 'mor', '##si', 'and', 'other', 'brotherhood', 'leaders', 'were', 'arrested', 'jan', '.', '28', 'and', 'held', 'in', 'the', 'wadi', 'nat', '##rou', '##n', 'prison', 'north', 'of', 'cairo', ',', 'until', 'they', 'escaped', 'two', 'days', 'later', '.', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     mor             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##si            | True: 0 | Pred: 0 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     other           | True: 0 | Pred: 0 ‚úÖ\n",
      "     brotherhood     | True: 0 | Pred: 0 ‚úÖ\n",
      "     leaders         | True: 0 | Pred: 0 ‚úÖ\n",
      "     were            | True: 0 | Pred: 0 ‚úÖ\n",
      "     arrested        | True: 0 | Pred: 0 ‚úÖ\n",
      "     jan             | True: 1 | Pred: 1 ‚úÖ\n",
      "     .               | True: 1 | Pred: 1 ‚úÖ\n",
      "     28              | True: 1 | Pred: 1 ‚úÖ\n",
      "     and             | True: 0 | Pred: 0 ‚úÖ\n",
      "     held            | True: 0 | Pred: 0 ‚úÖ\n",
      "     in              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     wadi            | True: 0 | Pred: 0 ‚úÖ\n",
      "     nat             | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##rou           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ##n             | True: 0 | Pred: 0 ‚úÖ\n",
      "     prison          | True: 0 | Pred: 0 ‚úÖ\n",
      "     north           | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     cairo           | True: 0 | Pred: 0 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     until           | True: 0 | Pred: 0 ‚úÖ\n",
      "     they            | True: 0 | Pred: 0 ‚úÖ\n",
      "     escaped         | True: 0 | Pred: 0 ‚úÖ\n",
      "     two             | True: 0 | Pred: 1 ‚ùå\n",
      "     days            | True: 0 | Pred: 1 ‚ùå\n",
      "     later           | True: 0 | Pred: 1 ‚ùå\n",
      "     .               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n",
      "\n",
      "üîπ Sentence 5:\n",
      "   Text: [CLS] ` on the side of the bloody germans during the war, were n ' t they? [SEP]\n",
      "   Tokens: ['[CLS]', '`', 'on', 'the', 'side', 'of', 'the', 'bloody', 'germans', 'during', 'the', 'war', ',', 'were', 'n', \"'\", 't', 'they', '?', '[SEP]']\n",
      "   True Labels:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Predicted Labels:[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   Comparison:\n",
      "     [CLS]           | True: 0 | Pred: 0 ‚úÖ\n",
      "     `               | True: 0 | Pred: 0 ‚úÖ\n",
      "     on              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     side            | True: 0 | Pred: 0 ‚úÖ\n",
      "     of              | True: 0 | Pred: 0 ‚úÖ\n",
      "     the             | True: 0 | Pred: 0 ‚úÖ\n",
      "     bloody          | True: 0 | Pred: 0 ‚úÖ\n",
      "     germans         | True: 0 | Pred: 0 ‚úÖ\n",
      "     during          | True: 1 | Pred: 1 ‚úÖ\n",
      "     the             | True: 1 | Pred: 1 ‚úÖ\n",
      "     war             | True: 1 | Pred: 1 ‚úÖ\n",
      "     ,               | True: 0 | Pred: 0 ‚úÖ\n",
      "     were            | True: 0 | Pred: 0 ‚úÖ\n",
      "     n               | True: 0 | Pred: 0 ‚úÖ\n",
      "     '               | True: 0 | Pred: 0 ‚úÖ\n",
      "     t               | True: 0 | Pred: 0 ‚úÖ\n",
      "     they            | True: 0 | Pred: 0 ‚úÖ\n",
      "     ?               | True: 0 | Pred: 0 ‚úÖ\n",
      "     [SEP]           | True: 0 | Pred: 0 ‚úÖ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dataloader):\n\u001b[32m      7\u001b[39m     input_ids, attention_mask, target_index = [item.to(device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     probs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     preds = torch.argmax(torch.softmax(probs, dim=-\u001b[32m1\u001b[39m), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_ids.size(\u001b[32m0\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mFrameElementClassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# Encode sentence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         sentence_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m         token_embeddings = sentence_outputs.last_hidden_state  \u001b[38;5;66;03m# shape: (B, T, H)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m        # Encode role label (like \"Time\" or \"Manner\")\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m        role_output = self.bert(input_ids=role_ids, attention_mask=role_mask)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m        role_embedding = role_output.last_hidden_state[:, 0, :]  # [CLS] token: shape (B, H)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m        role_embedding = self.query_encoder(role_embedding)  # shape: (B, H)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ogboi\\NLPProject\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_examples_to_print = 5  # or however many you want\n",
    "examples_printed = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        input_ids, attention_mask, target_index = [item.to(device) for item in batch]\n",
    "        probs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(torch.softmax(probs, dim=-1), dim=-1)\n",
    "\n",
    "        for j in range(input_ids.size(0)):\n",
    "            if examples_printed >= num_examples_to_print:\n",
    "                break\n",
    "\n",
    "            input_id = input_ids[j]\n",
    "            attention = attention_mask[j]\n",
    "            pred = preds[j]\n",
    "            label = target_index[j]\n",
    "\n",
    "            # Only consider real (non-padding) tokens\n",
    "            mask = (attention == 1) & (label != -100)\n",
    "            input_id = input_id[mask]\n",
    "            pred = pred[mask]\n",
    "            label = label[mask]\n",
    "\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "            sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "            print(f\"\\nüîπ Sentence {examples_printed + 1}:\")\n",
    "            print(f\"   Text: {sentence}\")\n",
    "            print(f\"   Tokens: {tokens}\")\n",
    "            print(f\"   True Labels:     {label.tolist()}\")\n",
    "            print(f\"   Predicted Labels:{pred.tolist()}\")\n",
    "\n",
    "            # Optional: highlight mismatches\n",
    "            print(\"   Comparison:\")\n",
    "            for tok, gold, guess in zip(tokens, label.tolist(), pred.tolist()):\n",
    "                status = \"‚úÖ\" if gold == guess else \"‚ùå\"\n",
    "                print(f\"     {tok:15} | True: {gold} | Pred: {guess} {status}\")\n",
    "\n",
    "            examples_printed += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_binary_spans(label_seq):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, val in enumerate(label_seq):\n",
    "        if val == 1 and start is None:\n",
    "            start = i\n",
    "        elif val == 0 and start is not None:\n",
    "            spans.append((start, i - 1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(label_seq) - 1))\n",
    "    return spans\n",
    "\n",
    "def evaluate_binary_predictions(true_labels_list, pred_labels_list):\n",
    "    strict_match = 0\n",
    "    partial_match = 0\n",
    "    total_spans = 0\n",
    "\n",
    "    for true_seq, pred_seq in zip(true_labels_list, pred_labels_list):\n",
    "        true_spans = extract_binary_spans(true_seq)\n",
    "        pred_spans = extract_binary_spans(pred_seq)\n",
    "        total_spans += len(true_spans)\n",
    "\n",
    "        for t_start, t_end in true_spans:\n",
    "            t_range = set(range(t_start, t_end + 1))\n",
    "            match_found = False\n",
    "            for p_start, p_end in pred_spans:\n",
    "                p_range = set(range(p_start, p_end + 1))\n",
    "                if t_range == p_range:\n",
    "                    strict_match += 1\n",
    "                    match_found = True\n",
    "                    break\n",
    "                elif t_range & p_range:\n",
    "                    match_found = True\n",
    "            if match_found:\n",
    "                partial_match += 1\n",
    "\n",
    "    return {\n",
    "        \"Total Time Elements\": total_spans,\n",
    "        \"Strict Matches\": strict_match,\n",
    "        \"Partial Matches\": partial_match,\n",
    "        \"Strict Accuracy\": strict_match / total_spans if total_spans > 0 else 0,\n",
    "        \"Partial Accuracy\": partial_match / total_spans if total_spans > 0 else 0\n",
    "    }\n",
    "\n",
    "# ‚¨áÔ∏è EVALUATION CODE\n",
    "def evaluate_model(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels_all = []\n",
    "    pred_labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=-1)  # shape: (B, T)\n",
    "\n",
    "            for label_seq, pred_seq, mask in zip(labels, predictions, attention_mask):\n",
    "                # Remove padding (-100) and apply attention mask\n",
    "                true_seq = [label.item() for label, m in zip(label_seq, mask) if m == 1 and label != -100]\n",
    "                pred_seq = [pred.item() for pred, m in zip(pred_seq, mask) if m == 1]\n",
    "\n",
    "                true_labels_all.append(true_seq)\n",
    "                pred_labels_all.append(pred_seq[:len(true_seq)])  # Match lengths just in case\n",
    "\n",
    "    return evaluate_binary_predictions(true_labels_all, pred_labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 902/902 [03:14<00:00,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Results:\n",
      "Total Time Elements: 4612\n",
      "Strict Matches: 3393\n",
      "Partial Matches: 4221\n",
      "Strict Accuracy: 0.736\n",
      "Partial Accuracy: 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model, val_dataloader, device)\n",
    "print(\"üìä Evaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
